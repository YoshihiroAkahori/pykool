{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBRP Dry Protocol - ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹è§£æè‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Eric Koolãƒ©ãƒœã®è«–æ–‡ã€ŒReactivity-based RNA profiling for analyzing transcriptome interactions of small molecules in human cellsã€ã®ãƒ‰ãƒ©ã‚¤ãƒ—ãƒ­ãƒˆã‚³ãƒ«éƒ¨åˆ†ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚\n",
    "\n",
    "**å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼**: ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹åˆå­¦è€…  \n",
    "**å…¥åŠ›**: FASTQãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒšã‚¢ã‚¨ãƒ³ãƒ‰å¯¾å¿œï¼‰  \n",
    "**å‡ºåŠ›**: RNA-è–¬ç‰©ç›¸äº’ä½œç”¨è§£æçµæœ\n",
    "\n",
    "## ğŸ”§ ãƒ¢ãƒ€ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼å¯¾å¿œ\n",
    "æœ€è¿‘ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼ï¼ˆIllumina NovaSeqã€NextSeqç­‰ï¼‰ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ãŒè‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "- **ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹**: ãƒãƒ¼ã‚³ãƒ¼ãƒ‰åˆ†é›¢æ¸ˆã¿FASTQãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›\n",
    "- **ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°**: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é…åˆ—é™¤å»æ¸ˆã¿ã®å‡ºåŠ›\n",
    "- **å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: ä½å“è³ªãƒªãƒ¼ãƒ‰é™¤å»æ¸ˆã¿ã®å‡ºåŠ›\n",
    "\n",
    "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ã¯ã€ã“ã‚Œã‚‰ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é«˜é€ŸåŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "1. è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "2. ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¿…è¦ã«å¿œã˜ã¦èª¿æ•´\n",
    "3. ã€ŒRun All Cellsã€ã§å…¨è‡ªå‹•å®Ÿè¡Œ\n",
    "4. çµæœã¯`data/results/`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¬ RBRP Dry Protocol Pipeline Started\n",
      "ğŸ“ Project Root: /home/akahod3f/work/kool\n",
      "ğŸ“ Log File: /home/akahod3f/work/kool/logs/rbrp_pipeline_20250917_044525.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "log_file = PROJECT_ROOT / 'logs' / f'rbrp_pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"ğŸ§¬ RBRP Dry Protocol Pipeline Started\")\n",
    "print(f\"ğŸ“ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“ Log File: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š =====\n# ã“ã“ã§å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„\n\n# å…¥åŠ›FASTQãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒšã‚¢ã‚¨ãƒ³ãƒ‰å¯¾å¿œï¼‰\n# Koolè«–æ–‡ã®SRA accession numbers (GSE229331) ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ä½¿ç”¨\n# download_rbrp_data.pyã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸå ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ï¼ˆdata/rawï¼‰\nINPUT_FASTQ_FILES = {\n    'SRR22397001': {  # HEK293_Probe2_only_rep1\n        'R1': 'data/raw/SRR22397001_1.fastq',\n        'R2': 'data/raw/SRR22397001_2.fastq'\n    },\n    'SRR22397002': {  # HEK293_Probe2_only_rep2\n        'R1': 'data/raw/SRR22397002_1.fastq',\n        'R2': 'data/raw/SRR22397002_2.fastq'\n    },\n    'SRR22397003': {  # HEK293_Probe2_Levofloxacin_rep1\n        'R1': 'data/raw/SRR22397003_1.fastq',\n        'R2': 'data/raw/SRR22397003_2.fastq'\n    },\n    'SRR22397004': {  # HEK293_Probe2_Levofloxacin_rep2\n        'R1': 'data/raw/SRR22397004_1.fastq',\n        'R2': 'data/raw/SRR22397004_2.fastq'\n    },\n    'SRR22397005': {  # HEK293_DMSO_ctrl_rep1\n        'R1': 'data/raw/SRR22397005_1.fastq',\n        'R2': 'data/raw/SRR22397005_2.fastq'\n    },\n    'SRR22397006': {  # HEK293_DMSO_ctrl_rep2\n        'R1': 'data/raw/SRR22397006_1.fastq',\n        'R2': 'data/raw/SRR22397006_2.fastq'\n    },\n    'SRR22397007': {  # HEK293_Levofloxacin_ctrl_rep1\n        'R1': 'data/raw/SRR22397007_1.fastq',\n        'R2': 'data/raw/SRR22397007_2.fastq'\n    },\n    'SRR22397008': {  # HEK293_Levofloxacin_ctrl_rep2\n        'R1': 'data/raw/SRR22397008_1.fastq',\n        'R2': 'data/raw/SRR22397008_2.fastq'\n    }\n}\n\n# å‚ç…§ã‚²ãƒãƒ ãƒ•ã‚¡ã‚¤ãƒ«\nREFERENCE_FILES = {\n    'genome_fasta': '/path/to/genome.fa',\n    'genome_gtf': '/path/to/genome.gtf',\n    'transcriptome_index': '/path/to/transcriptome_index',\n    'adapter_fasta': '/path/to/adapter.fa'\n}\n\n# ãƒãƒ¼ã‚³ãƒ¼ãƒ‰æƒ…å ±ï¼ˆdemultiplexã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„å ´åˆã®ã¿ä½¿ç”¨ï¼‰\nBARCODES = {\n    'SRR22397001': 'GGTT',  # HEK293_Probe2_only_rep1\n    'SRR22397002': 'TTGT',  # HEK293_Probe2_only_rep2\n    'SRR22397003': 'ACCT',  # HEK293_Probe2_Levofloxacin_rep1\n    'SRR22397004': 'CAAT',  # HEK293_Probe2_Levofloxacin_rep2\n    'SRR22397005': 'GCAA',  # HEK293_DMSO_ctrl_rep1\n    'SRR22397006': 'AATC',  # HEK293_DMSO_ctrl_rep2\n    'SRR22397007': 'TGAC',  # HEK293_Levofloxacin_ctrl_rep1\n    'SRR22397008': 'CGGT'   # HEK293_Levofloxacin_ctrl_rep2\n}\n\n# è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nANALYSIS_PARAMS = {\n    'min_read_length': 25,\n    'min_rpkm': 1,\n    'min_sequencing_depth': 200,\n    'rbrp_score_threshold': 0.12,\n    'p_value_threshold': 0.05\n}\n\n# ğŸ”§ å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³\nPROCESSING_OPTIONS = {\n    'skip_demultiplex': True,            # True: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—\n    'skip_adapter_trimming': True,       # True: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—\n    'skip_pcr_duplicate_removal': False, # True: PCRé‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—\n    'perform_quality_control': False,    # True: FastQCã«ã‚ˆã‚‹å“è³ªç¢ºèªã‚’å®Ÿè¡Œ\n    'adapter_sequences': {               # ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é…åˆ—ï¼ˆtrimmingå®Ÿè¡Œæ™‚ã®ã¿ä½¿ç”¨ï¼‰\n        'R1': 'AGATCGGAAGAGCGGTTCAG',\n        'R2': 'AGATCGGAAGAGCGGTTCAG'\n    }\n}\n\nprint(\"âœ… è¨­å®šå®Œäº†ï¼ˆSRA accession numbersä½¿ç”¨ãƒ»data/rawãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ»ãƒšã‚¢ã‚¨ãƒ³ãƒ‰å¯¾å¿œãƒ»ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰\")\nprint(f\"ğŸ“Š è§£æå¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(INPUT_FASTQ_FILES)}\")\nprint(f\"ğŸ“ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: data/raw (download_rbrp_data.pyã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)\")\nprint(f\"ğŸ”§ ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚¹ã‚­ãƒƒãƒ—: {PROCESSING_OPTIONS['skip_demultiplex']}\")\nprint(f\"ğŸ”§ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚¹ã‚­ãƒƒãƒ—: {PROCESSING_OPTIONS['skip_adapter_trimming']}\")\nlogger.info(f\"Analysis started with {len(INPUT_FASTQ_FILES)} paired-end samples from Kool lab (GSE229331) in data/raw\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ãƒ»å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_external_tools():\n",
    "    \"\"\"å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã®å­˜åœ¨ç¢ºèª\"\"\"\n",
    "    required_tools = [\n",
    "        'fastqc',\n",
    "        'bowtie2',\n",
    "        'gffread',\n",
    "        'wiggletools',\n",
    "        'bedGraphToBigWig',\n",
    "        'wigToBigWig'\n",
    "    ]\n",
    "    \n",
    "    missing_tools = []\n",
    "    \n",
    "    for tool in required_tools:\n",
    "        try:\n",
    "            result = subprocess.run(['which', tool], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… {tool}: {result.stdout.strip()}\")\n",
    "            else:\n",
    "                missing_tools.append(tool)\n",
    "                print(f\"âŒ {tool}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        except Exception as e:\n",
    "            missing_tools.append(tool)\n",
    "            print(f\"âŒ {tool}: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "    \n",
    "    if missing_tools:\n",
    "        print(f\"\\nâš ï¸ ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_tools)}\")\n",
    "        print(\"ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯README.mdã‚’å‚ç…§ã—ã¦ãã ã•ã„\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nğŸ‰ ã™ã¹ã¦ã®å¿…è¦ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™\")\n",
    "        return True\n",
    "\n",
    "tools_available = check_external_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ç¾¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(cmd, description, check=True):\n",
    "    \"\"\"ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\"\"\"\n",
    "    logger.info(f\"å®Ÿè¡Œä¸­: {description}\")\n",
    "    logger.debug(f\"ã‚³ãƒãƒ³ãƒ‰: {cmd}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, check=check, \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            logger.info(f\"âœ… å®Œäº†: {description}\")\n",
    "            return result\n",
    "        else:\n",
    "            logger.error(f\"âŒ å¤±æ•—: {description}\")\n",
    "            logger.error(f\"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›: {result.stderr}\")\n",
    "            if check:\n",
    "                raise subprocess.CalledProcessError(result.returncode, cmd)\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ ä¾‹å¤–ç™ºç”Ÿ: {description} - {e}\")\n",
    "        if check:\n",
    "            raise\n",
    "        return None\n",
    "\n",
    "def find_latest_processed_files(sample_name, file_type=\"fastq\"):\n",
    "    \"\"\"\n",
    "    ã‚µãƒ³ãƒ—ãƒ«ã®æœ€æ–°å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹•çš„ã«æ¤œç´¢\n",
    "    \n",
    "    Args:\n",
    "        sample_name (str): ã‚µãƒ³ãƒ—ãƒ«å\n",
    "        file_type (str): ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ— (\"fastq\", \"sam\", \"rt\" ãªã©)\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"R1\": path, \"R2\": path} ã¾ãŸã¯ None\n",
    "    \"\"\"\n",
    "    search_patterns = {\n",
    "        \"fastq\": [\n",
    "            f\"data/processed/trimmed/{sample_name}_trimmed\",  # ãƒˆãƒªãƒŸãƒ³ã‚°æ¸ˆã¿\n",
    "            f\"data/processed/{sample_name}_rmdup\",            # PCRé‡è¤‡é™¤å»æ¸ˆã¿\n",
    "            f\"data/processed/{sample_name}_demux\",            # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿\n",
    "        ],\n",
    "        \"sam\": [\n",
    "            f\"data/processed/aligned/{sample_name}.sam\"\n",
    "        ],\n",
    "        \"rt\": [\n",
    "            f\"data/processed/aligned/{sample_name}.rt\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if file_type == \"fastq\":\n",
    "        # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ\n",
    "        for pattern in search_patterns[file_type]:\n",
    "            r1_file = PROJECT_ROOT / f\"{pattern}_1.fastq\"\n",
    "            r2_file = PROJECT_ROOT / f\"{pattern}_2.fastq\"\n",
    "            \n",
    "            if r1_file.exists() and r2_file.exists():\n",
    "                logger.info(f\"âœ… {sample_name}: ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ« {pattern}_*.fastq\")\n",
    "                return {\"R1\": r1_file, \"R2\": r2_file}\n",
    "        \n",
    "        # æœ€å¾Œã®æ‰‹æ®µï¼šå…ƒã®FASTQãƒ•ã‚¡ã‚¤ãƒ«\n",
    "        original_r1 = INPUT_FASTQ_FILES[sample_name]['R1']\n",
    "        original_r2 = INPUT_FASTQ_FILES[sample_name]['R2']\n",
    "        if Path(original_r1).exists() and Path(original_r2).exists():\n",
    "            logger.warning(f\"âš ï¸ {sample_name}: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ {original_r1}, {original_r2}\")\n",
    "            return {\"R1\": Path(original_r1), \"R2\": Path(original_r2)}\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ\n",
    "        for pattern in search_patterns.get(file_type, []):\n",
    "            file_path = PROJECT_ROOT / pattern\n",
    "            if file_path.exists():\n",
    "                logger.info(f\"âœ… {sample_name}: ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ« {pattern}\")\n",
    "                return file_path\n",
    "        return None\n",
    "\n",
    "def create_output_dirs():\n",
    "    \"\"\"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\"\"\"\n",
    "    dirs = [\n",
    "        'logs',  # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ \n",
    "        'data/processed/fastqc',\n",
    "        'data/processed/trimmed', \n",
    "        'data/processed/aligned',\n",
    "        'data/processed/rbrp_scores',\n",
    "        'data/results/bigwig',\n",
    "        'data/results/figures'\n",
    "    ]\n",
    "    \n",
    "    for dir_path in dirs:\n",
    "        (PROJECT_ROOT / dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "\n",
    "create_output_dirs()\n",
    "print(\"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ã‚¹ãƒ†ãƒƒãƒ—1: å“è³ªç®¡ç†ãƒ»ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_quality_control_and_demultiplex():\n",
    "    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—1: å“è³ªç®¡ç†ã¨ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"\\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—1: å“è³ªç®¡ç†ãƒ»ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹é–‹å§‹\")\n",
    "    \n",
    "    # ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª\n",
    "    skip_demux = PROCESSING_OPTIONS.get('skip_demultiplex', False)\n",
    "    perform_qc = PROCESSING_OPTIONS.get('perform_quality_control', True)\n",
    "    \n",
    "    if skip_demux:\n",
    "        print(\"ğŸš€ ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€è¿‘ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼ã§ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ï¼‰\")\n",
    "    if not perform_qc:\n",
    "        print(\"ğŸš€ å“è³ªç®¡ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "    \n",
    "    for sample_name, fastq_paths in tqdm(INPUT_FASTQ_FILES.items(), desc=\"Processing samples\"):\n",
    "        # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
    "        r1_path = fastq_paths['R1']\n",
    "        r2_path = fastq_paths['R2']\n",
    "        \n",
    "        if not Path(r1_path).exists():\n",
    "            logger.warning(f\"âš ï¸ R1ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {r1_path}\")\n",
    "            continue\n",
    "        if not Path(r2_path).exists():\n",
    "            logger.warning(f\"âš ï¸ R2ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {r2_path}\")\n",
    "            continue\n",
    "        \n",
    "        # FastQCå®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if perform_qc:\n",
    "            fastqc_cmd_r1 = f\"fastqc -o {PROJECT_ROOT}/data/processed/fastqc {r1_path}\"\n",
    "            fastqc_cmd_r2 = f\"fastqc -o {PROJECT_ROOT}/data/processed/fastqc {r2_path}\"\n",
    "            \n",
    "            run_command(fastqc_cmd_r1, f\"FastQC for {sample_name} R1\")\n",
    "            run_command(fastqc_cmd_r2, f\"FastQC for {sample_name} R2\")\n",
    "        else:\n",
    "            print(f\"â­ï¸ {sample_name}: FastQCã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ\")\n",
    "        \n",
    "        # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if not skip_demux and sample_name in BARCODES:\n",
    "            barcode = BARCODES[sample_name]\n",
    "            output_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "            output_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "            \n",
    "            # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒãƒ¼ã‚³ãƒ¼ãƒ‰æŠ½å‡º\n",
    "            demux_cmd_r1 = f\"\"\"grep -A 3 \"^@.*{barcode}\" {r1_path} | grep -v \"^--$\" > {output_r1}\"\"\"\n",
    "            demux_cmd_r2 = f\"\"\"grep -A 3 \"^@.*{barcode}\" {r2_path} | grep -v \"^--$\" > {output_r2}\"\"\"\n",
    "            \n",
    "            run_command(demux_cmd_r1, f\"Demultiplexing {sample_name} R1\", check=False)\n",
    "            run_command(demux_cmd_r2, f\"Demultiplexing {sample_name} R2\", check=False)\n",
    "            \n",
    "            logger.info(f\"âœ… {sample_name}: ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹å®Œäº†\")\n",
    "        elif skip_demux:\n",
    "            # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã€å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "            output_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "            output_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "            \n",
    "            # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤ï¼‰\n",
    "            if output_r1.exists():\n",
    "                output_r1.unlink()\n",
    "            if output_r2.exists():\n",
    "                output_r2.unlink()\n",
    "                \n",
    "            output_r1.symlink_to(Path(r1_path).absolute())\n",
    "            output_r2.symlink_to(Path(r2_path).absolute())\n",
    "            \n",
    "            logger.info(f\"âœ… {sample_name}: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆï¼‰\")\n",
    "        else:\n",
    "            logger.warning(f\"âš ï¸ {sample_name}: ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    skip_msg = []\n",
    "    if skip_demux:\n",
    "        skip_msg.append(\"ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹\")\n",
    "    if not perform_qc:\n",
    "        skip_msg.append(\"å“è³ªç®¡ç†\")\n",
    "    \n",
    "    completion_msg = \"âœ… ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: å“è³ªç®¡ç†ãƒ»ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹\"\n",
    "    if skip_msg:\n",
    "        completion_msg += f\"ï¼ˆ{', '.join(skip_msg)}ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰\"\n",
    "    \n",
    "    print(completion_msg)\n",
    "\n",
    "step1_quality_control_and_demultiplex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ã‚¹ãƒ†ãƒƒãƒ—2: PCRé‡è¤‡é™¤å»ãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2_remove_duplicates_and_trim():\n",
    "    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—2: PCRé‡è¤‡é™¤å»ã¨ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"\\nâœ‚ï¸ ã‚¹ãƒ†ãƒƒãƒ—2: PCRé‡è¤‡é™¤å»ãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°é–‹å§‹ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n",
    "    \n",
    "    # ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª\n",
    "    skip_trimming = PROCESSING_OPTIONS.get('skip_adapter_trimming', False)\n",
    "    skip_pcr_removal = PROCESSING_OPTIONS.get('skip_pcr_duplicate_removal', False)\n",
    "    \n",
    "    if skip_trimming:\n",
    "        print(\"ğŸš€ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€è¿‘ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼ã§ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ï¼‰\")\n",
    "    if skip_pcr_removal:\n",
    "        print(\"ğŸš€ PCRé‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "    \n",
    "    for sample_name in tqdm(INPUT_FASTQ_FILES.keys(), desc=\"Processing trimming\"):\n",
    "        # å‰ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹•çš„ã«æ¤œç´¢ï¼ˆãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿ï¼‰\n",
    "        input_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "        input_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "        \n",
    "        if not input_r1.exists() or not input_r2.exists():\n",
    "            logger.warning(f\"âš ï¸ {sample_name}: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "            original_r1 = Path(INPUT_FASTQ_FILES[sample_name]['R1'])\n",
    "            original_r2 = Path(INPUT_FASTQ_FILES[sample_name]['R2'])\n",
    "            if original_r1.exists() and original_r2.exists():\n",
    "                input_r1, input_r2 = original_r1, original_r2\n",
    "                logger.info(f\"ğŸ“„ {sample_name}: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\")\n",
    "            else:\n",
    "                logger.error(f\"âŒ {sample_name}: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "                continue\n",
    "        \n",
    "        # PCRé‡è¤‡é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if not skip_pcr_removal:\n",
    "            rmdup_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_rmdup_1.fastq\"\n",
    "            rmdup_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_rmdup_2.fastq\"\n",
    "            \n",
    "            # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰PCRé‡è¤‡é™¤å»ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
    "            rmdup_cmd_r1 = f\"\"\"awk '/^@/ {{if (seen[$0]++) next}} 1' {input_r1} > {rmdup_r1}\"\"\"\n",
    "            rmdup_cmd_r2 = f\"\"\"awk '/^@/ {{if (seen[$0]++) next}} 1' {input_r2} > {rmdup_r2}\"\"\"\n",
    "            \n",
    "            run_command(rmdup_cmd_r1, f\"Remove duplicates for {sample_name} R1\")\n",
    "            run_command(rmdup_cmd_r2, f\"Remove duplicates for {sample_name} R2\")\n",
    "            \n",
    "            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°\n",
    "            trim_input_r1 = rmdup_r1\n",
    "            trim_input_r2 = rmdup_r2\n",
    "        else:\n",
    "            print(f\"â­ï¸ {sample_name}: PCRé‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ\")\n",
    "            trim_input_r1 = input_r1\n",
    "            trim_input_r2 = input_r2\n",
    "        \n",
    "        # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        trimmed_r1 = PROJECT_ROOT / f\"data/processed/trimmed/{sample_name}_trimmed_1.fastq\"\n",
    "        trimmed_r2 = PROJECT_ROOT / f\"data/processed/trimmed/{sample_name}_trimmed_2.fastq\"\n",
    "        \n",
    "        if not skip_trimming:\n",
    "            # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é™¤å»ã¨ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒˆãƒªãƒŸãƒ³ã‚°\n",
    "            adapter_r1 = PROCESSING_OPTIONS['adapter_sequences']['R1']\n",
    "            adapter_r2 = PROCESSING_OPTIONS['adapter_sequences']['R2']\n",
    "            \n",
    "            trim_cmd = f\"\"\"cutadapt -a {adapter_r1} -A {adapter_r2} \\\n",
    "                          -q 30 -m {ANALYSIS_PARAMS['min_read_length']} \\\n",
    "                          -o {trimmed_r1} -p {trimmed_r2} \\\n",
    "                          {trim_input_r1} {trim_input_r2}\"\"\"\n",
    "            \n",
    "            run_command(trim_cmd, f\"Paired-end adapter trimming for {sample_name}\", check=False)\n",
    "        else:\n",
    "            # ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã€ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "            if trimmed_r1.exists():\n",
    "                trimmed_r1.unlink()\n",
    "            if trimmed_r2.exists():\n",
    "                trimmed_r2.unlink()\n",
    "                \n",
    "            trimmed_r1.symlink_to(trim_input_r1.absolute())\n",
    "            trimmed_r2.symlink_to(trim_input_r2.absolute())\n",
    "            \n",
    "            print(f\"â­ï¸ {sample_name}: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆï¼‰\")\n",
    "        \n",
    "        logger.info(f\"âœ… {sample_name}: å‡¦ç†å®Œäº†\")\n",
    "    \n",
    "    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ\n",
    "    skip_msg = []\n",
    "    if skip_trimming:\n",
    "        skip_msg.append(\"ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°\")\n",
    "    if skip_pcr_removal:\n",
    "        skip_msg.append(\"PCRé‡è¤‡é™¤å»\")\n",
    "    \n",
    "    completion_msg = \"âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: PCRé‡è¤‡é™¤å»ãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\"\n",
    "    if skip_msg:\n",
    "        completion_msg += f\"ï¼ˆ{', '.join(skip_msg)}ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰\"\n",
    "    \n",
    "    print(completion_msg)\n",
    "\n",
    "step2_remove_duplicates_and_trim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ã‚¹ãƒ†ãƒƒãƒ—3: é…åˆ—ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»è»¢å†™ç”£ç‰©ç™ºç¾é‡è¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3_alignment_and_expression():\n",
    "    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—3: é…åˆ—ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨è»¢å†™ç”£ç‰©ç™ºç¾é‡è¨ˆç®—ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"\\nğŸ§¬ ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»ç™ºç¾é‡è¨ˆç®—é–‹å§‹ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n",
    "    \n",
    "    for sample_name in tqdm(INPUT_FASTQ_FILES.keys(), desc=\"Processing alignment\"):\n",
    "        # å‹•çš„ã«æœ€æ–°ã®å‡¦ç†æ¸ˆã¿FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢\n",
    "        fastq_files = find_latest_processed_files(sample_name, \"fastq\")\n",
    "        \n",
    "        if not fastq_files:\n",
    "            logger.error(f\"âŒ {sample_name}: å‡¦ç†æ¸ˆã¿FASTQãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            continue\n",
    "            \n",
    "        trimmed_r1 = fastq_files[\"R1\"]\n",
    "        trimmed_r2 = fastq_files[\"R2\"]\n",
    "        \n",
    "        if not trimmed_r1.exists():\n",
    "            logger.warning(f\"âš ï¸ R1ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {trimmed_r1}\")\n",
    "            continue\n",
    "        if not trimmed_r2.exists():\n",
    "            logger.warning(f\"âš ï¸ R2ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {trimmed_r2}\")\n",
    "            continue\n",
    "        \n",
    "        # Bowtie2ã§ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ\n",
    "        sam_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.sam\"\n",
    "        \n",
    "        # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ-1, -2ã§ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šï¼‰\n",
    "        align_cmd = f\"\"\"bowtie2 -1 {trimmed_r1} -2 {trimmed_r2} -S {sam_file} \\\n",
    "                       -x {REFERENCE_FILES['transcriptome_index']} \\\n",
    "                       --non-deterministic --time \\\n",
    "                       --minins 50 --maxins 500 \\\n",
    "                       --no-mixed --no-discordant\"\"\"\n",
    "        \n",
    "        run_command(align_cmd, f\"Paired-end alignment for {sample_name}\")\n",
    "        \n",
    "        # RPKMè¨ˆç®—\n",
    "        rpkm_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.rpkm\"\n",
    "        rpkm_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rpkm.py -i {sam_file} -o {rpkm_file}\"\"\"\n",
    "        \n",
    "        run_command(rpkm_cmd, f\"RPKM calculation for {sample_name}\", check=False)\n",
    "        \n",
    "        # RTstopè¨ˆç®—\n",
    "        rt_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.rt\"\n",
    "        rt_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rtstops.py \\\n",
    "                    -i {sam_file} -o {rt_file} -r {rpkm_file} -c {ANALYSIS_PARAMS['min_rpkm']}\"\"\"\n",
    "        \n",
    "        run_command(rt_cmd, f\"RTstop calculation for {sample_name}\", check=False)\n",
    "        logger.info(f\"âœ… {sample_name}: ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»ç™ºç¾é‡è¨ˆç®—å®Œäº†\")\n",
    "    \n",
    "    print(\"âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»ç™ºç¾é‡è¨ˆç®—ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n",
    "\n",
    "step3_alignment_and_expression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ã‚¹ãƒ†ãƒƒãƒ—4: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ»çµ±è¨ˆè§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step4_rbrp_score_calculation():\n    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—4: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ã¨çµ±è¨ˆè§£æï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\"\"\"\n    print(\"\\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ»çµ±è¨ˆè§£æé–‹å§‹ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n    \n    # SRA accession numbersã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘\n    # GSE229331ã®Koolè«–æ–‡ãƒ‡ãƒ¼ã‚¿æ§‹æˆã«åŸºã¥ã\n    probe_samples = [\n        'SRR22397001',  # HEK293_Probe2_only_rep1\n        'SRR22397002',  # HEK293_Probe2_only_rep2\n        'SRR22397003',  # HEK293_Probe2_Levofloxacin_rep1\n        'SRR22397004'   # HEK293_Probe2_Levofloxacin_rep2\n    ]\n    \n    control_samples = [\n        'SRR22397005',  # HEK293_DMSO_ctrl_rep1\n        'SRR22397006',  # HEK293_DMSO_ctrl_rep2\n        'SRR22397007',  # HEK293_Levofloxacin_ctrl_rep1\n        'SRR22397008'   # HEK293_Levofloxacin_ctrl_rep2\n    ]\n    \n    print(f\"ğŸ“‹ ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«: {probe_samples}\")\n    print(f\"ğŸ“‹ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚µãƒ³ãƒ—ãƒ«: {control_samples}\")\n    \n    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰RTstopãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸\n    if len(control_samples) >= 2:\n        control_files = []\n        for sample in control_samples:\n            rt_file = find_latest_processed_files(sample, \"rt\")\n            if rt_file and rt_file.exists():\n                control_files.append(str(rt_file))\n            else:\n                logger.warning(f\"âš ï¸ {sample}: RTãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n        \n        if control_files:\n            merged_control = PROJECT_ROOT / \"data/processed/rbrp_scores/merged_control.rt\"\n            merge_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/merge_rt_files.py \\\n                           -i {':'.join(control_files)} -o {merged_control}\"\"\"\n            run_command(merge_cmd, \"Merging control RTstop files\", check=False)\n    \n    # å„ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®RBRPã‚¹ã‚³ã‚¢è¨ˆç®—\n    for sample_name in tqdm(probe_samples, desc=\"Calculating RBRP scores\"):\n        rt_file = find_latest_processed_files(sample_name, \"rt\")\n        \n        if not rt_file or not rt_file.exists():\n            logger.warning(f\"âš ï¸ {sample_name}: RTãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n            continue\n        \n        # RTãƒ•ã‚¡ã‚¤ãƒ«æ­£è¦åŒ–\n        normalized_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_normalized.rt\"\n        norm_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/normalize_rt_file.py \\\n                      -i {rt_file} -o {normalized_file} -d 32 -l 32\"\"\"\n        run_command(norm_cmd, f\"Normalizing RT file for {sample_name}\", check=False)\n        \n        # RBRPã‚¹ã‚³ã‚¢è¨ˆç®—\n        rbrp_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_rbrp.out\"\n        background_file = merged_control if 'merged_control' in locals() else None\n        \n        if background_file and background_file.exists():\n            rbrp_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rbrp_score.py \\\n                          -f {normalized_file} -b {background_file} -o {rbrp_file} \\\n                          -e dividing -y 0.5\"\"\"\n        else:\n            rbrp_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rbrp_score.py \\\n                          -f {normalized_file} -o {rbrp_file}\"\"\"\n        \n        run_command(rbrp_cmd, f\"Calculating RBRP scores for {sample_name}\", check=False)\n        \n        # ä½å“è³ªã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n        filtered_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n        filter_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/filter_rbrp_scores.py \\\n                        -i {rbrp_file} -o {filtered_file} \\\n                        -t {ANALYSIS_PARAMS['min_sequencing_depth']} -s 5 -e 30\"\"\"\n        run_command(filter_cmd, f\"Filtering RBRP scores for {sample_name}\", check=False)\n        \n        logger.info(f\"âœ… {sample_name}: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—å®Œäº†\")\n    \n    print(\"âœ… ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ»çµ±è¨ˆè§£æï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n\nstep4_rbrp_score_calculation()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ã‚¹ãƒ†ãƒƒãƒ—5: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµæœå‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step5_visualization_and_output():\n    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—5: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã¨çµæœå‡ºåŠ›\"\"\"\n    print(\"\\nğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—5: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµæœå‡ºåŠ›é–‹å§‹\")\n    \n    # ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’å®šç¾©ï¼ˆSRA accession numbersåŸºæº–ï¼‰\n    probe_samples = [\n        'SRR22397001',  # HEK293_Probe2_only_rep1\n        'SRR22397002',  # HEK293_Probe2_only_rep2\n        'SRR22397003',  # HEK293_Probe2_Levofloxacin_rep1\n        'SRR22397004'   # HEK293_Probe2_Levofloxacin_rep2\n    ]\n    \n    for sample_name in tqdm(probe_samples, desc=\"Generating visualization files\"):\n        filtered_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n        \n        if not filtered_file.exists():\n            logger.warning(f\"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filtered_file}\")\n            continue\n        \n        # bedgraphãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ\n        bedgraph_file = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}.bedgraph\"\n        bedgraph_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/generate_bedgraph.py \\\n                          -i {filtered_file} -o {bedgraph_file} \\\n                          -g {REFERENCE_FILES['genome_gtf']} \\\n                          -a {REFERENCE_FILES.get('transcriptome_fasta', '')}\"\"\"\n        run_command(bedgraph_cmd, f\"Generating bedgraph for {sample_name}\", check=False)\n        \n        # bigwigãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆUCscãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n        bigwig_file = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}.bw\"\n        genome_size_file = PROJECT_ROOT / \"data/genome.size\"  # äº‹å‰ã«æº–å‚™ãŒå¿…è¦\n        \n        if bedgraph_file.exists():\n            # ã‚½ãƒ¼ãƒˆã¨é‡è¤‡é™¤å»\n            sorted_bedgraph = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}_sorted.bedgraph\"\n            sort_cmd = f\"sort -k1,1 -k2,3n {bedgraph_file} | uniq > {sorted_bedgraph}\"\n            run_command(sort_cmd, f\"Sorting bedgraph for {sample_name}\", check=False)\n            \n            # bigwigå¤‰æ›\n            if genome_size_file.exists():\n                bw_cmd = f\"bedGraphToBigWig {sorted_bedgraph} {genome_size_file} {bigwig_file}\"\n                run_command(bw_cmd, f\"Converting to bigwig for {sample_name}\", check=False)\n        \n        logger.info(f\"âœ… {sample_name}: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†\")\n    \n    print(\"âœ… ã‚¹ãƒ†ãƒƒãƒ—5å®Œäº†: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµæœå‡ºåŠ›\")\n\nstep5_visualization_and_output()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. çµæœã‚µãƒãƒªãƒ¼ãƒ»çµ±è¨ˆæƒ…å ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"çµæœã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"\\nğŸ“‹ çµæœã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for sample_name in INPUT_FASTQ_FILES.keys():\n",
    "        sample_summary = {'sample_name': sample_name}\n",
    "        \n",
    "        # å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢çµæœã‚’è¨˜éŒ²\n",
    "        fastq_files = find_latest_processed_files(sample_name, \"fastq\")\n",
    "        sam_file = find_latest_processed_files(sample_name, \"sam\")\n",
    "        rt_file = find_latest_processed_files(sample_name, \"rt\")\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n",
    "        sample_summary['input_files_exist'] = bool(fastq_files)\n",
    "        sample_summary['aligned_exists'] = bool(sam_file and sam_file.exists())\n",
    "        sample_summary['rt_analysis_exists'] = bool(rt_file and rt_file.exists())\n",
    "        \n",
    "        # RBRP ã‚¹ã‚³ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª\n",
    "        rbrp_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n",
    "        sample_summary['rbrp_scores_exists'] = rbrp_file.exists()\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±\n",
    "        if fastq_files:\n",
    "            r1_size = fastq_files[\"R1\"].stat().st_size / (1024**2) if fastq_files[\"R1\"].exists() else 0\n",
    "            r2_size = fastq_files[\"R2\"].stat().st_size / (1024**2) if fastq_files[\"R2\"].exists() else 0\n",
    "            sample_summary['input_size_mb'] = round(r1_size + r2_size, 2)\n",
    "            sample_summary['processing_method'] = str(fastq_files[\"R1\"]).split('/')[-1].split('_')[1]  # demux, rmdup, trimmed\n",
    "        else:\n",
    "            sample_summary['input_size_mb'] = 0\n",
    "            sample_summary['processing_method'] = 'none'\n",
    "            \n",
    "        if sam_file and sam_file.exists():\n",
    "            sample_summary['aligned_size_mb'] = round(sam_file.stat().st_size / (1024**2), 2)\n",
    "        else:\n",
    "            sample_summary['aligned_size_mb'] = 0\n",
    "            \n",
    "        if rbrp_file.exists():\n",
    "            sample_summary['rbrp_size_mb'] = round(rbrp_file.stat().st_size / (1024**2), 2)\n",
    "        else:\n",
    "            sample_summary['rbrp_size_mb'] = 0\n",
    "        \n",
    "        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œæˆåº¦\n",
    "        completed_steps = sum([\n",
    "            sample_summary['input_files_exist'],\n",
    "            sample_summary['aligned_exists'], \n",
    "            sample_summary['rt_analysis_exists'],\n",
    "            sample_summary['rbrp_scores_exists']\n",
    "        ])\n",
    "        sample_summary['pipeline_completion'] = f\"{completed_steps}/4\"\n",
    "        sample_summary['pipeline_success'] = completed_steps == 4\n",
    "        \n",
    "        summary_data.append(sample_summary)\n",
    "    \n",
    "    # ã‚µãƒãƒªãƒ¼DataFrameä½œæˆ\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # çµæœè¡¨ç¤º\n",
    "    print(\"\\nğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "    summary_file = PROJECT_ROOT / \"data/results/processing_summary_dynamic.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    successful_samples = summary_df['pipeline_success'].sum()\n",
    "    total_samples = len(summary_df)\n",
    "    \n",
    "    # å‡¦ç†æ–¹æ³•åˆ¥ã®çµ±è¨ˆ\n",
    "    processing_methods = summary_df['processing_method'].value_counts()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ å‡¦ç†çµ±è¨ˆï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰:\")\n",
    "    print(f\"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}\")\n",
    "    print(f\"   ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œå…¨æˆåŠŸ: {successful_samples}\")\n",
    "    print(f\"   æˆåŠŸç‡: {successful_samples/total_samples*100:.1f}%\")\n",
    "    print(f\"   å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {summary_df['input_size_mb'].mean():.1f} MB\")\n",
    "    print(f\"   ä½¿ç”¨ã•ã‚ŒãŸå‡¦ç†æ–¹æ³•:\")\n",
    "    for method, count in processing_methods.items():\n",
    "        print(f\"     - {method}: {count} ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "    print(f\"   ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {summary_file}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "summary_report = generate_summary_report()\n",
    "\n",
    "# å®Ÿè¡Œæ™‚é–“ã®è¨˜éŒ²\n",
    "print(f\"\\nğŸ‰ å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†!\")\n",
    "print(f\"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:\")\n",
    "print(f\"   - å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {PROJECT_ROOT}/data/processed/\")\n",
    "print(f\"   - æœ€çµ‚çµæœ: {PROJECT_ROOT}/data/results/\")\n",
    "print(f\"   - ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}\")\n",
    "\n",
    "logger.info(\"RBRP Dry Protocol Pipeline (Dynamic File Resolution) completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. çµæœå¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization_plots():\n",
    "    \"\"\"çµæœã®å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ\"\"\"\n",
    "    print(\"\\nğŸ“Š çµæœå¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ\")\n",
    "    \n",
    "    # å‡¦ç†ã‚µãƒãƒªãƒ¼ã®å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RBRP Dry Protocol - å‡¦ç†çµæœã‚µãƒãƒªãƒ¼', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†å¸ƒ\n",
    "    size_columns = [col for col in summary_report.columns if 'size_mb' in col]\n",
    "    if size_columns:\n",
    "        size_data = summary_report[size_columns].fillna(0)\n",
    "        axes[0, 0].bar(range(len(size_columns)), size_data.mean(), \n",
    "                      tick_label=[col.replace('_size_mb', '') for col in size_columns])\n",
    "        axes[0, 0].set_title('å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º (MB)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. å‡¦ç†æˆåŠŸç‡\n",
    "    success_columns = [col for col in summary_report.columns if 'exists' in col]\n",
    "    if success_columns:\n",
    "        success_rates = summary_report[success_columns].mean() * 100\n",
    "        axes[0, 1].bar(range(len(success_rates)), success_rates.values,\n",
    "                      tick_label=[col.replace('_exists', '') for col in success_columns])\n",
    "        axes[0, 1].set_title('å‡¦ç†æˆåŠŸç‡ (%)')\n",
    "        axes[0, 1].set_ylim(0, 100)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. ã‚µãƒ³ãƒ—ãƒ«åˆ¥å‡¦ç†çŠ¶æ³\n",
    "    if success_columns:\n",
    "        sample_success = summary_report[success_columns].sum(axis=1)\n",
    "        axes[1, 0].bar(range(len(sample_success)), sample_success.values,\n",
    "                      tick_label=summary_report['sample_name'])\n",
    "        axes[1, 0].set_title('ã‚µãƒ³ãƒ—ãƒ«åˆ¥å®Œäº†ã‚¹ãƒ†ãƒƒãƒ—æ•°')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. å‡¦ç†æ™‚é–“ã®ç›®å®‰ï¼ˆä»®æƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "    processing_steps = ['ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹', 'ãƒˆãƒªãƒŸãƒ³ã‚°', 'ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ', 'RBRPè¨ˆç®—', 'å¯è¦–åŒ–']\n",
    "    estimated_times = [5, 10, 30, 20, 5]  # åˆ†å˜ä½\n",
    "    axes[1, 1].bar(processing_steps, estimated_times)\n",
    "    axes[1, 1].set_title('ã‚¹ãƒ†ãƒƒãƒ—åˆ¥æ¨å®šå‡¦ç†æ™‚é–“ (åˆ†)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # å›³ã‚’ä¿å­˜\n",
    "    plot_file = PROJECT_ROOT / \"data/results/figures/processing_summary.png\"\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ“Š å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {plot_file}\")\n",
    "\n",
    "# å¯è¦–åŒ–å®Ÿè¡Œ\n",
    "try:\n",
    "    create_visualization_plots()\n",
    "except Exception as e:\n",
    "    logger.warning(f\"å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"âš ï¸ å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªä½“ã¯æ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã¾ã™\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}