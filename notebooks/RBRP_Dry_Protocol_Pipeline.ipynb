{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBRP Dry Protocol - バイオインフォマティクス解析自動化パイプライン\n",
    "\n",
    "このノートブックは、Eric Koolラボの論文「Reactivity-based RNA profiling for analyzing transcriptome interactions of small molecules in human cells」のドライプロトコル部分を自動化します。\n",
    "\n",
    "**対象ユーザー**: バイオインフォマティクス初学者  \n",
    "**入力**: FASTQファイル（ペアエンド対応）  \n",
    "**出力**: RNA-薬物相互作用解析結果\n",
    "\n",
    "## 🔧 モダンシーケンサー対応\n",
    "最近のシーケンサー（Illumina NovaSeq、NextSeq等）では以下の処理が自動で実行されるため、スキップオプションを用意しています：\n",
    "\n",
    "- **デマルチプレックス**: バーコード分離済みFASTQファイルの出力\n",
    "- **アダプタートリミング**: アダプター配列除去済みの出力\n",
    "- **品質フィルタリング**: 低品質リード除去済みの出力\n",
    "\n",
    "デフォルト設定では、これらの処理をスキップして高速化されています。\n",
    "\n",
    "## 使用方法\n",
    "1. 設定セクションで入力ファイルパスを指定\n",
    "2. スキップオプションを必要に応じて調整\n",
    "3. 「Run All Cells」で全自動実行\n",
    "4. 結果は`data/results/`フォルダに保存されます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定と初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 RBRP Dry Protocol Pipeline Started\n",
      "📁 Project Root: /home/akahod3f/work/kool\n",
      "📝 Log File: /home/akahod3f/work/kool/logs/rbrp_pipeline_20250917_044525.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# プロジェクトルートディレクトリを設定\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# ログ設定\n",
    "log_file = PROJECT_ROOT / 'logs' / f'rbrp_pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"🧬 RBRP Dry Protocol Pipeline Started\")\n",
    "print(f\"📁 Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"📝 Log File: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 設定ファイル読み込み・入力ファイル指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🔧 処理スキップオプション\nPROCESSING_OPTIONS = {\n    'skip_demultiplex': True,            # True: デマルチプレックスをスキップ\n    'skip_adapter_trimming': True,       # True: アダプタートリミングをスキップ\n    'skip_pcr_duplicate_removal': False, # True: PCR重複除去をスキップ\n    'perform_quality_control': False,    # True: FastQCによる品質確認を実行\n    'adapter_sequences': {               # カスタムアダプター配列（trimming実行時のみ使用）\n        'R1': 'AGATCGGAAGAGCGGTTCAG',\n        'R2': 'AGATCGGAAGAGCGGTTCAG'\n    },\n    # 💾 ディスク容量管理オプション\n    'cleanup_intermediate_fastq': True,  # True: 中間FASTQファイルを自動削除してディスク容量節約\n    'preserve_original_fastq': True,     # True: 元のダウンロード/シーケンスファイルは保持\n    'cleanup_confirmation': True         # True: 削除前に安全性確認\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 依存関係チェック・外部ツール確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_external_tools():\n",
    "    \"\"\"外部ツールの存在確認\"\"\"\n",
    "    required_tools = [\n",
    "        'fastqc',\n",
    "        'bowtie2',\n",
    "        'gffread',\n",
    "        'wiggletools',\n",
    "        'bedGraphToBigWig',\n",
    "        'wigToBigWig'\n",
    "    ]\n",
    "    \n",
    "    missing_tools = []\n",
    "    \n",
    "    for tool in required_tools:\n",
    "        try:\n",
    "            result = subprocess.run(['which', tool], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"✅ {tool}: {result.stdout.strip()}\")\n",
    "            else:\n",
    "                missing_tools.append(tool)\n",
    "                print(f\"❌ {tool}: 見つかりません\")\n",
    "        except Exception as e:\n",
    "            missing_tools.append(tool)\n",
    "            print(f\"❌ {tool}: エラー - {e}\")\n",
    "    \n",
    "    if missing_tools:\n",
    "        print(f\"\\n⚠️ 以下のツールが不足しています: {', '.join(missing_tools)}\")\n",
    "        print(\"インストール方法はREADME.mdを参照してください\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n🎉 すべての必要ツールが利用可能です\")\n",
    "        return True\n",
    "\n",
    "tools_available = check_external_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. パイプライン実行関数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_command(cmd, description, check=True):\n    \"\"\"コマンド実行のヘルパー関数\"\"\"\n    logger.info(f\"実行中: {description}\")\n    logger.debug(f\"コマンド: {cmd}\")\n    \n    try:\n        result = subprocess.run(cmd, shell=True, check=check, \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            logger.info(f\"✅ 完了: {description}\")\n            return result\n        else:\n            logger.error(f\"❌ 失敗: {description}\")\n            logger.error(f\"エラー出力: {result.stderr}\")\n            if check:\n                raise subprocess.CalledProcessError(result.returncode, cmd)\n            return result\n    except Exception as e:\n        logger.error(f\"❌ 例外発生: {description} - {e}\")\n        if check:\n            raise\n        return None\n\ndef is_original_fastq_file(file_path, sample_id):\n    \"\"\"\n    元のFASTQファイル（ダウンロード/シーケンスファイル）かどうか判定\n    \n    Args:\n        file_path (Path): ファイルパス\n        sample_id (str): サンプルID\n    \n    Returns:\n        bool: 元ファイルの場合True\n    \"\"\"\n    if sample_id not in INPUT_FASTQ_FILES:\n        return False\n        \n    original_r1 = Path(INPUT_FASTQ_FILES[sample_id]['R1']).resolve()\n    original_r2 = Path(INPUT_FASTQ_FILES[sample_id]['R2']).resolve()\n    file_path_resolved = Path(file_path).resolve()\n    \n    return file_path_resolved == original_r1 or file_path_resolved == original_r2\n\ndef safe_cleanup_fastq_files(files_to_remove, sample_id, step_description):\n    \"\"\"\n    中間FASTQファイルを安全に削除\n    \n    Args:\n        files_to_remove (list): 削除対象ファイルリスト\n        sample_id (str): サンプルID\n        step_description (str): 処理ステップの説明\n    \n    Returns:\n        dict: 削除結果の統計\n    \"\"\"\n    if not PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False):\n        return {'skipped': True, 'reason': 'cleanup disabled'}\n    \n    cleanup_stats = {\n        'files_checked': 0,\n        'files_removed': 0,\n        'space_freed_mb': 0,\n        'preserved_originals': 0,\n        'errors': []\n    }\n    \n    for file_path in files_to_remove:\n        file_path = Path(file_path)\n        cleanup_stats['files_checked'] += 1\n        \n        if not file_path.exists():\n            continue\n            \n        # 元ファイル保護チェック\n        if PROCESSING_OPTIONS.get('preserve_original_fastq', True):\n            if is_original_fastq_file(file_path, sample_id):\n                cleanup_stats['preserved_originals'] += 1\n                logger.info(f\"🔒 元ファイル保護: {file_path.name}\")\n                continue\n        \n        # シンボリックリンクの場合は削除（実ファイルは削除しない）\n        if file_path.is_symlink():\n            try:\n                file_size_mb = 0  # シンボリックリンクなのでサイズはカウントしない\n                file_path.unlink()\n                cleanup_stats['files_removed'] += 1\n                logger.info(f\"🗑️ シンボリックリンク削除: {file_path.name}\")\n            except Exception as e:\n                cleanup_stats['errors'].append(f\"シンボリックリンク削除失敗 {file_path}: {e}\")\n            continue\n        \n        # 安全性確認\n        if PROCESSING_OPTIONS.get('cleanup_confirmation', True):\n            # ファイルが中間処理ファイルかどうか確認\n            is_intermediate = any(keyword in str(file_path) for keyword in \n                                ['_demux', '_rmdup', '_trimmed', 'processed/'])\n            \n            if not is_intermediate:\n                logger.warning(f\"⚠️ 削除スキップ（中間ファイルでない可能性）: {file_path}\")\n                continue\n        \n        # ファイルサイズ記録\n        try:\n            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n            \n            # ファイル削除実行\n            file_path.unlink()\n            cleanup_stats['files_removed'] += 1\n            cleanup_stats['space_freed_mb'] += file_size_mb\n            \n            logger.info(f\"🗑️ 削除完了: {file_path.name} ({file_size_mb:.1f} MB)\")\n            \n        except Exception as e:\n            cleanup_stats['errors'].append(f\"削除失敗 {file_path}: {e}\")\n            logger.error(f\"❌ ファイル削除エラー: {file_path} - {e}\")\n    \n    # クリーンアップサマリー\n    if cleanup_stats['files_removed'] > 0:\n        logger.info(f\"💾 {step_description} - クリーンアップ完了: \"\n                   f\"{cleanup_stats['files_removed']}ファイル削除, \"\n                   f\"{cleanup_stats['space_freed_mb']:.1f}MB節約\")\n    \n    return cleanup_stats\n\ndef get_condition_replicates(condition_name):\n    \"\"\"\n    実験条件のreplicate一覧を取得\n    \n    Args:\n        condition_name (str): 実験条件名\n    \n    Returns:\n        list: replicate ID一覧\n    \"\"\"\n    if condition_name in EXPERIMENT_DESIGN['conditions']:\n        return EXPERIMENT_DESIGN['conditions'][condition_name]['replicates']\n    return []\n\ndef get_samples_by_type(sample_type):\n    \"\"\"\n    サンプルタイプ別にreplicate一覧を取得\n    \n    Args:\n        sample_type (str): 'probe' または 'control'\n    \n    Returns:\n        list: replicate ID一覧\n    \"\"\"\n    samples = []\n    for condition_name, condition_info in EXPERIMENT_DESIGN['conditions'].items():\n        if condition_info['type'] == sample_type:\n            samples.extend(condition_info['replicates'])\n    return samples\n\ndef get_condition_from_sample(sample_id):\n    \"\"\"\n    サンプルIDから実験条件を取得\n    \n    Args:\n        sample_id (str): サンプルID\n    \n    Returns:\n        str: 実験条件名\n    \"\"\"\n    if sample_id in INPUT_FASTQ_FILES:\n        return INPUT_FASTQ_FILES[sample_id]['condition']\n    return None\n\ndef merge_replicate_files(condition_name, file_pattern, output_file, method='mean'):\n    \"\"\"\n    同一条件のreplicateファイルを統計的にマージ\n    \n    Args:\n        condition_name (str): 実験条件名\n        file_pattern (str): ファイルパターン ('rt', 'rpkm', 'rbrp' など)\n        output_file (Path): 出力ファイルパス\n        method (str): 統計手法 ('mean', 'median', 'sum')\n    \n    Returns:\n        bool: 成功/失敗\n    \"\"\"\n    replicates = get_condition_replicates(condition_name)\n    \n    if len(replicates) < EXPERIMENT_DESIGN['replicate_handling']['min_replicates']:\n        logger.warning(f\"⚠️ {condition_name}: replicate数不足 ({len(replicates)} < {EXPERIMENT_DESIGN['replicate_handling']['min_replicates']})\")\n        return False\n    \n    # 各replicateのファイルパス取得\n    input_files = []\n    for replicate_id in replicates:\n        if file_pattern == 'rt':\n            file_path = find_latest_processed_files(replicate_id, \"rt\")\n        elif file_pattern == 'rpkm':\n            file_path = PROJECT_ROOT / f\"data/processed/aligned/{replicate_id}.rpkm\"\n        elif file_pattern == 'rbrp':\n            file_path = PROJECT_ROOT / f\"data/processed/rbrp_scores/{replicate_id}_filtered.out\"\n        else:\n            continue\n            \n        if file_path and Path(file_path).exists():\n            input_files.append(str(file_path))\n        else:\n            logger.warning(f\"⚠️ {replicate_id}: {file_pattern}ファイルが見つかりません\")\n    \n    if len(input_files) < 2:\n        logger.warning(f\"⚠️ {condition_name}: 有効な{file_pattern}ファイルが不足\")\n        return False\n    \n    # Pythonスクリプトでreplicate統合\n    merge_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/merge_replicates.py \\\n                   -i {':'.join(input_files)} -o {output_file} \\\n                   -m {method} -c {condition_name}\"\"\"\n    \n    result = run_command(merge_cmd, f\"Merging {file_pattern} files for {condition_name}\", check=False)\n    \n    if result and result.returncode == 0:\n        logger.info(f\"✅ {condition_name}: {file_pattern}ファイルマージ完了 (N={len(input_files)})\")\n        return True\n    else:\n        logger.error(f\"❌ {condition_name}: {file_pattern}ファイルマージ失敗\")\n        return False\n\ndef detect_outliers_in_condition(condition_name, file_pattern='rpkm'):\n    \"\"\"\n    同一条件内でのreplicate外れ値検出\n    \n    Args:\n        condition_name (str): 実験条件名\n        file_pattern (str): 解析対象ファイルパターン\n    \n    Returns:\n        list: 外れ値と判定されたreplicate ID一覧\n    \"\"\"\n    if not EXPERIMENT_DESIGN['replicate_handling']['outlier_detection']:\n        return []\n    \n    replicates = get_condition_replicates(condition_name)\n    if len(replicates) < 3:  # 外れ値検出には最低3サンプル必要\n        return []\n    \n    # 簡易統計解析による外れ値検出のロジックを実装\n    # （実際の実装では、より詳細な統計解析が必要）\n    outliers = []\n    \n    logger.info(f\"📊 {condition_name}: 外れ値検出実行中 (N={len(replicates)})\")\n    # ここに統計解析ロジックを追加\n    \n    return outliers\n\ndef validate_replicate_structure():\n    \"\"\"\n    実験設計のreplicate構造を検証\n    \n    Returns:\n        dict: 検証結果\n    \"\"\"\n    validation_results = {\n        'valid': True,\n        'warnings': [],\n        'errors': [],\n        'summary': {}\n    }\n    \n    for condition_name, condition_info in EXPERIMENT_DESIGN['conditions'].items():\n        replicate_count = len(condition_info['replicates'])\n        validation_results['summary'][condition_name] = {\n            'replicate_count': replicate_count,\n            'type': condition_info['type']\n        }\n        \n        if replicate_count < EXPERIMENT_DESIGN['replicate_handling']['min_replicates']:\n            validation_results['warnings'].append(\n                f\"{condition_name}: replicate数が最小要件を下回っています (N={replicate_count})\"\n            )\n        \n        # ファイル存在確認\n        missing_files = []\n        for replicate_id in condition_info['replicates']:\n            if replicate_id in INPUT_FASTQ_FILES:\n                r1_path = INPUT_FASTQ_FILES[replicate_id]['R1']\n                r2_path = INPUT_FASTQ_FILES[replicate_id]['R2']\n                if not Path(r1_path).exists():\n                    missing_files.append(f\"{replicate_id}_R1\")\n                if not Path(r2_path).exists():\n                    missing_files.append(f\"{replicate_id}_R2\")\n        \n        if missing_files:\n            validation_results['errors'].append(\n                f\"{condition_name}: FASTQファイルが見つかりません: {', '.join(missing_files)}\"\n            )\n            validation_results['valid'] = False\n    \n    return validation_results\n\ndef find_latest_processed_files(sample_name, file_type=\"fastq\"):\n    \"\"\"\n    サンプルの最新処理済みファイルを動的に検索\n    \n    Args:\n        sample_name (str): サンプル名\n        file_type (str): ファイルタイプ (\"fastq\", \"sam\", \"rt\" など)\n    \n    Returns:\n        dict: {\"R1\": path, \"R2\": path} または Path または None\n    \"\"\"\n    search_patterns = {\n        \"fastq\": [\n            f\"data/processed/trimmed/{sample_name}_trimmed\",  # トリミング済み\n            f\"data/processed/{sample_name}_rmdup\",            # PCR重複除去済み\n            f\"data/processed/{sample_name}_demux\",            # デマルチプレックス済み\n        ],\n        \"sam\": [\n            f\"data/processed/aligned/{sample_name}.sam\"\n        ],\n        \"rt\": [\n            f\"data/processed/aligned/{sample_name}.rt\"\n        ]\n    }\n    \n    if file_type == \"fastq\":\n        # ペアエンドファイルの場合\n        for pattern in search_patterns[file_type]:\n            r1_file = PROJECT_ROOT / f\"{pattern}_1.fastq\"\n            r2_file = PROJECT_ROOT / f\"{pattern}_2.fastq\"\n            \n            if r1_file.exists() and r2_file.exists():\n                logger.info(f\"✅ {sample_name}: 使用ファイル {pattern}_*.fastq\")\n                return {\"R1\": r1_file, \"R2\": r2_file}\n        \n        # 最後の手段：元のFASTQファイル\n        if sample_name in INPUT_FASTQ_FILES:\n            original_r1 = INPUT_FASTQ_FILES[sample_name]['R1']\n            original_r2 = INPUT_FASTQ_FILES[sample_name]['R2']\n            if Path(original_r1).exists() and Path(original_r2).exists():\n                logger.warning(f\"⚠️ {sample_name}: 元ファイルを使用 {original_r1}, {original_r2}\")\n                return {\"R1\": Path(original_r1), \"R2\": Path(original_r2)}\n            \n        return None\n    \n    else:\n        # 単一ファイルの場合\n        for pattern in search_patterns.get(file_type, []):\n            file_path = PROJECT_ROOT / pattern\n            if file_path.exists():\n                logger.info(f\"✅ {sample_name}: 使用ファイル {pattern}\")\n                return file_path\n        return None\n\ndef create_output_dirs():\n    \"\"\"出力ディレクトリ作成\"\"\"\n    dirs = [\n        'logs',  # ログディレクトリを追加\n        'data/processed/fastqc',\n        'data/processed/trimmed', \n        'data/processed/aligned',\n        'data/processed/rbrp_scores',\n        'data/processed/merged_conditions',  # replicate統合結果\n        'data/results/bigwig',\n        'data/results/figures',\n        'data/results/statistical_analysis'  # 統計解析結果\n    ]\n    \n    for dir_path in dirs:\n        (PROJECT_ROOT / dir_path).mkdir(parents=True, exist_ok=True)\n    \n    logger.info(\"📁 出力ディレクトリを作成しました\")\n\n# 実験設計の検証実行\nvalidation_results = validate_replicate_structure()\ncreate_output_dirs()\n\nprint(\"📁 出力ディレクトリ準備完了\")\n\n# クリーンアップ設定の表示\nif PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False):\n    print(\"💾 ディスク容量管理: ✅ 中間FASTQファイル自動削除有効\")\n    print(f\"🔒 元ファイル保護: {'✅ 有効' if PROCESSING_OPTIONS.get('preserve_original_fastq', True) else '❌ 無効'}\")\nelse:\n    print(\"💾 ディスク容量管理: ❌ 中間FASTQファイル削除無効（手動削除が必要）\")\n\nprint(\"\\n🔬 実験設計検証結果:\")\nif validation_results['valid']:\n    print(\"✅ 実験設計は有効です\")\nelse:\n    print(\"❌ 実験設計にエラーがあります\")\n\nfor warning in validation_results['warnings']:\n    print(f\"⚠️ 警告: {warning}\")\n\nfor error in validation_results['errors']:\n    print(f\"❌ エラー: {error}\")\n\nprint(f\"\\n📊 実験設計サマリー:\")\nfor condition, info in validation_results['summary'].items():\n    print(f\"  🧪 {condition}: N={info['replicate_count']} ({info['type']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ステップ1: 品質管理・デマルチプレックス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_quality_control_and_demultiplex():\n",
    "    \"\"\"ステップ1: 品質管理とデマルチプレックス（スキップオプション対応）\"\"\"\n",
    "    print(\"\\n🔍 ステップ1: 品質管理・デマルチプレックス開始\")\n",
    "    \n",
    "    # スキップオプションの確認\n",
    "    skip_demux = PROCESSING_OPTIONS.get('skip_demultiplex', False)\n",
    "    perform_qc = PROCESSING_OPTIONS.get('perform_quality_control', True)\n",
    "    \n",
    "    if skip_demux:\n",
    "        print(\"🚀 デマルチプレックスをスキップ（最近のシーケンサーでは既に実行済み）\")\n",
    "    if not perform_qc:\n",
    "        print(\"🚀 品質管理をスキップ\")\n",
    "    \n",
    "    for sample_name, fastq_paths in tqdm(INPUT_FASTQ_FILES.items(), desc=\"Processing samples\"):\n",
    "        # ペアエンドファイルの存在確認\n",
    "        r1_path = fastq_paths['R1']\n",
    "        r2_path = fastq_paths['R2']\n",
    "        \n",
    "        if not Path(r1_path).exists():\n",
    "            logger.warning(f\"⚠️ R1ファイルが見つかりません: {r1_path}\")\n",
    "            continue\n",
    "        if not Path(r2_path).exists():\n",
    "            logger.warning(f\"⚠️ R2ファイルが見つかりません: {r2_path}\")\n",
    "            continue\n",
    "        \n",
    "        # FastQC実行（オプション）\n",
    "        if perform_qc:\n",
    "            fastqc_cmd_r1 = f\"fastqc -o {PROJECT_ROOT}/data/processed/fastqc {r1_path}\"\n",
    "            fastqc_cmd_r2 = f\"fastqc -o {PROJECT_ROOT}/data/processed/fastqc {r2_path}\"\n",
    "            \n",
    "            run_command(fastqc_cmd_r1, f\"FastQC for {sample_name} R1\")\n",
    "            run_command(fastqc_cmd_r2, f\"FastQC for {sample_name} R2\")\n",
    "        else:\n",
    "            print(f\"⏭️ {sample_name}: FastQCをスキップしました\")\n",
    "        \n",
    "        # デマルチプレックス（オプション）\n",
    "        if not skip_demux and sample_name in BARCODES:\n",
    "            barcode = BARCODES[sample_name]\n",
    "            output_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "            output_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "            \n",
    "            # ペアエンドバーコード抽出\n",
    "            demux_cmd_r1 = f\"\"\"grep -A 3 \"^@.*{barcode}\" {r1_path} | grep -v \"^--$\" > {output_r1}\"\"\"\n",
    "            demux_cmd_r2 = f\"\"\"grep -A 3 \"^@.*{barcode}\" {r2_path} | grep -v \"^--$\" > {output_r2}\"\"\"\n",
    "            \n",
    "            run_command(demux_cmd_r1, f\"Demultiplexing {sample_name} R1\", check=False)\n",
    "            run_command(demux_cmd_r2, f\"Demultiplexing {sample_name} R2\", check=False)\n",
    "            \n",
    "            logger.info(f\"✅ {sample_name}: ペアエンドデマルチプレックス完了\")\n",
    "        elif skip_demux:\n",
    "            # デマルチプレックスをスキップする場合、元ファイルへのシンボリックリンクを作成\n",
    "            output_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "            output_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "            \n",
    "            # シンボリックリンク作成（既に存在する場合は削除）\n",
    "            if output_r1.exists():\n",
    "                output_r1.unlink()\n",
    "            if output_r2.exists():\n",
    "                output_r2.unlink()\n",
    "                \n",
    "            output_r1.symlink_to(Path(r1_path).absolute())\n",
    "            output_r2.symlink_to(Path(r2_path).absolute())\n",
    "            \n",
    "            logger.info(f\"✅ {sample_name}: デマルチプレックスをスキップ（シンボリックリンク作成）\")\n",
    "        else:\n",
    "            logger.warning(f\"⚠️ {sample_name}: バーコードが指定されていません\")\n",
    "    \n",
    "    skip_msg = []\n",
    "    if skip_demux:\n",
    "        skip_msg.append(\"デマルチプレックス\")\n",
    "    if not perform_qc:\n",
    "        skip_msg.append(\"品質管理\")\n",
    "    \n",
    "    completion_msg = \"✅ ステップ1完了: 品質管理・デマルチプレックス\"\n",
    "    if skip_msg:\n",
    "        completion_msg += f\"（{', '.join(skip_msg)}をスキップ）\"\n",
    "    \n",
    "    print(completion_msg)\n",
    "\n",
    "step1_quality_control_and_demultiplex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ステップ2: PCR重複除去・トリミング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step2_remove_duplicates_and_trim():\n    \"\"\"ステップ2: PCR重複除去とトリミング（動的ファイル解決対応・ディスク容量管理）\"\"\"\n    print(\"\\n✂️ ステップ2: PCR重複除去・トリミング開始（動的ファイル解決対応・ディスク容量管理）\")\n    \n    # スキップオプションの確認\n    skip_trimming = PROCESSING_OPTIONS.get('skip_adapter_trimming', False)\n    skip_pcr_removal = PROCESSING_OPTIONS.get('skip_pcr_duplicate_removal', False)\n    \n    if skip_trimming:\n        print(\"🚀 アダプタートリミングをスキップ（最近のシーケンサーでは既に実行済み）\")\n    if skip_pcr_removal:\n        print(\"🚀 PCR重複除去をスキップ\")\n    \n    # 全体のクリーンアップ統計\n    total_cleanup_stats = {\n        'files_removed': 0,\n        'space_freed_mb': 0,\n        'preserved_originals': 0\n    }\n    \n    for sample_name in tqdm(INPUT_FASTQ_FILES.keys(), desc=\"Processing trimming\"):\n        # 前ステップのファイルを動的に検索（デマルチプレックス済み）\n        input_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n        input_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n        \n        if not input_r1.exists() or not input_r2.exists():\n            logger.warning(f\"⚠️ {sample_name}: デマルチプレックス済みファイルが見つかりません\")\n            # 元ファイルの確認\n            original_r1 = Path(INPUT_FASTQ_FILES[sample_name]['R1'])\n            original_r2 = Path(INPUT_FASTQ_FILES[sample_name]['R2'])\n            if original_r1.exists() and original_r2.exists():\n                input_r1, input_r2 = original_r1, original_r2\n                logger.info(f\"📄 {sample_name}: 元ファイルを使用\")\n            else:\n                logger.error(f\"❌ {sample_name}: 入力ファイルが見つかりません\")\n                continue\n        \n        # PCR重複除去（オプション）\n        files_to_cleanup_after_pcr = []\n        if not skip_pcr_removal:\n            rmdup_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_rmdup_1.fastq\"\n            rmdup_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_rmdup_2.fastq\"\n            \n            # ペアエンドPCR重複除去（簡易版）\n            rmdup_cmd_r1 = f\"\"\"awk '/^@/ {{if (seen[$0]++) next}} 1' {input_r1} > {rmdup_r1}\"\"\"\n            rmdup_cmd_r2 = f\"\"\"awk '/^@/ {{if (seen[$0]++) next}} 1' {input_r2} > {rmdup_r2}\"\"\"\n            \n            run_command(rmdup_cmd_r1, f\"Remove duplicates for {sample_name} R1\")\n            run_command(rmdup_cmd_r2, f\"Remove duplicates for {sample_name} R2\")\n            \n            # 次のステップの入力ファイルを更新\n            trim_input_r1 = rmdup_r1\n            trim_input_r2 = rmdup_r2\n            \n            # デマルチプレックス済みファイルをクリーンアップ対象に追加（元ファイルでない場合）\n            if not is_original_fastq_file(input_r1, sample_name):\n                files_to_cleanup_after_pcr.append(input_r1)\n            if not is_original_fastq_file(input_r2, sample_name):\n                files_to_cleanup_after_pcr.append(input_r2)\n        else:\n            print(f\"⏭️ {sample_name}: PCR重複除去をスキップしました\")\n            trim_input_r1 = input_r1\n            trim_input_r2 = input_r2\n        \n        # アダプタートリミング（オプション）\n        trimmed_r1 = PROJECT_ROOT / f\"data/processed/trimmed/{sample_name}_trimmed_1.fastq\"\n        trimmed_r2 = PROJECT_ROOT / f\"data/processed/trimmed/{sample_name}_trimmed_2.fastq\"\n        \n        files_to_cleanup_after_trim = []\n        if not skip_trimming:\n            # ペアエンドアダプター除去とクオリティトリミング\n            adapter_r1 = PROCESSING_OPTIONS['adapter_sequences']['R1']\n            adapter_r2 = PROCESSING_OPTIONS['adapter_sequences']['R2']\n            \n            trim_cmd = f\"\"\"cutadapt -a {adapter_r1} -A {adapter_r2} \\\n                          -q 30 -m {ANALYSIS_PARAMS['min_read_length']} \\\n                          -o {trimmed_r1} -p {trimmed_r2} \\\n                          {trim_input_r1} {trim_input_r2}\"\"\"\n            \n            run_command(trim_cmd, f\"Paired-end adapter trimming for {sample_name}\", check=False)\n            \n            # PCR重複除去済みファイルをクリーンアップ対象に追加\n            if not skip_pcr_removal:\n                files_to_cleanup_after_trim.extend([trim_input_r1, trim_input_r2])\n        else:\n            # トリミングをスキップする場合、シンボリックリンクを作成\n            if trimmed_r1.exists():\n                trimmed_r1.unlink()\n            if trimmed_r2.exists():\n                trimmed_r2.unlink()\n                \n            trimmed_r1.symlink_to(trim_input_r1.absolute())\n            trimmed_r2.symlink_to(trim_input_r2.absolute())\n            \n            print(f\"⏭️ {sample_name}: アダプタートリミングをスキップ（シンボリックリンク作成）\")\n        \n        # PCR重複除去後のクリーンアップ\n        if files_to_cleanup_after_pcr:\n            cleanup_stats = safe_cleanup_fastq_files(\n                files_to_cleanup_after_pcr, \n                sample_name, \n                f\"PCR重複除去完了後 ({sample_name})\"\n            )\n            if not cleanup_stats.get('skipped', False):\n                total_cleanup_stats['files_removed'] += cleanup_stats['files_removed']\n                total_cleanup_stats['space_freed_mb'] += cleanup_stats['space_freed_mb']\n                total_cleanup_stats['preserved_originals'] += cleanup_stats['preserved_originals']\n        \n        # トリミング後のクリーンアップ\n        if files_to_cleanup_after_trim:\n            cleanup_stats = safe_cleanup_fastq_files(\n                files_to_cleanup_after_trim, \n                sample_name, \n                f\"トリミング完了後 ({sample_name})\"\n            )\n            if not cleanup_stats.get('skipped', False):\n                total_cleanup_stats['files_removed'] += cleanup_stats['files_removed']\n                total_cleanup_stats['space_freed_mb'] += cleanup_stats['space_freed_mb']\n                total_cleanup_stats['preserved_originals'] += cleanup_stats['preserved_originals']\n        \n        logger.info(f\"✅ {sample_name}: 処理完了\")\n    \n    # 完了メッセージ作成\n    skip_msg = []\n    if skip_trimming:\n        skip_msg.append(\"アダプタートリミング\")\n    if skip_pcr_removal:\n        skip_msg.append(\"PCR重複除去\")\n    \n    completion_msg = \"✅ ステップ2完了: PCR重複除去・トリミング（動的ファイル解決対応・ディスク容量管理）\"\n    if skip_msg:\n        completion_msg += f\"（{', '.join(skip_msg)}をスキップ）\"\n    \n    print(completion_msg)\n    \n    # クリーンアップサマリー表示\n    if PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False) and total_cleanup_stats['files_removed'] > 0:\n        print(f\"💾 ステップ2クリーンアップサマリー:\")\n        print(f\"   🗑️ 削除ファイル数: {total_cleanup_stats['files_removed']}\")\n        print(f\"   💾 節約容量: {total_cleanup_stats['space_freed_mb']:.1f} MB\")\n        print(f\"   🔒 保護した元ファイル: {total_cleanup_stats['preserved_originals']}\")\n\nstep2_remove_duplicates_and_trim()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ステップ3: 配列アライメント・転写産物発現量計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3_alignment_and_expression():\n",
    "    \"\"\"ステップ3: 配列アライメントと転写産物発現量計算（動的ファイル解決対応）\"\"\"\n",
    "    print(\"\\n🧬 ステップ3: アライメント・発現量計算開始（動的ファイル解決対応）\")\n",
    "    \n",
    "    for sample_name in tqdm(INPUT_FASTQ_FILES.keys(), desc=\"Processing alignment\"):\n",
    "        # 動的に最新の処理済みFASTQファイルを検索\n",
    "        fastq_files = find_latest_processed_files(sample_name, \"fastq\")\n",
    "        \n",
    "        if not fastq_files:\n",
    "            logger.error(f\"❌ {sample_name}: 処理済みFASTQファイルが見つかりません\")\n",
    "            continue\n",
    "            \n",
    "        trimmed_r1 = fastq_files[\"R1\"]\n",
    "        trimmed_r2 = fastq_files[\"R2\"]\n",
    "        \n",
    "        if not trimmed_r1.exists():\n",
    "            logger.warning(f\"⚠️ R1ファイルが見つかりません: {trimmed_r1}\")\n",
    "            continue\n",
    "        if not trimmed_r2.exists():\n",
    "            logger.warning(f\"⚠️ R2ファイルが見つかりません: {trimmed_r2}\")\n",
    "            continue\n",
    "        \n",
    "        # Bowtie2でペアエンドアライメント\n",
    "        sam_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.sam\"\n",
    "        \n",
    "        # ペアエンドマッピング（-1, -2でペアエンドファイル指定）\n",
    "        align_cmd = f\"\"\"bowtie2 -1 {trimmed_r1} -2 {trimmed_r2} -S {sam_file} \\\n",
    "                       -x {REFERENCE_FILES['transcriptome_index']} \\\n",
    "                       --non-deterministic --time \\\n",
    "                       --minins 50 --maxins 500 \\\n",
    "                       --no-mixed --no-discordant\"\"\"\n",
    "        \n",
    "        run_command(align_cmd, f\"Paired-end alignment for {sample_name}\")\n",
    "        \n",
    "        # RPKM計算\n",
    "        rpkm_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.rpkm\"\n",
    "        rpkm_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rpkm.py -i {sam_file} -o {rpkm_file}\"\"\"\n",
    "        \n",
    "        run_command(rpkm_cmd, f\"RPKM calculation for {sample_name}\", check=False)\n",
    "        \n",
    "        # RTstop計算\n",
    "        rt_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.rt\"\n",
    "        rt_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rtstops.py \\\n",
    "                    -i {sam_file} -o {rt_file} -r {rpkm_file} -c {ANALYSIS_PARAMS['min_rpkm']}\"\"\"\n",
    "        \n",
    "        run_command(rt_cmd, f\"RTstop calculation for {sample_name}\", check=False)\n",
    "        logger.info(f\"✅ {sample_name}: ペアエンドアライメント・発現量計算完了\")\n",
    "    \n",
    "    print(\"✅ ステップ3完了: アライメント・発現量計算（動的ファイル解決対応）\")\n",
    "\n",
    "step3_alignment_and_expression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ステップ4: RBRPスコア計算・統計解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step4_rbrp_score_calculation():\n    \"\"\"ステップ4: RBRPスコア計算と統計解析（Replicate対応・動的ファイル解決）\"\"\"\n    print(\"\\n📊 ステップ4: RBRPスコア計算・統計解析開始（Replicate対応・動的ファイル解決）\")\n    \n    # 動的にサンプルグループを取得\n    probe_samples = get_samples_by_type('probe')\n    control_samples = get_samples_by_type('control')\n    \n    print(f\"📋 プローブサンプル: {probe_samples}\")\n    print(f\"📋 コントロールサンプル: {control_samples}\")\n    \n    # まず個別replicateのRBRPスコア計算\n    print(\"\\n🔬 個別replicateのRBRPスコア計算:\")\n    \n    # バックグラウンドRTstopファイルをマージ（コントロールサンプル）\n    if len(control_samples) >= 2:\n        control_files = []\n        for sample in control_samples:\n            rt_file = find_latest_processed_files(sample, \"rt\")\n            if rt_file and rt_file.exists():\n                control_files.append(str(rt_file))\n            else:\n                logger.warning(f\"⚠️ {sample}: RTファイルが見つかりません\")\n        \n        if control_files:\n            merged_control = PROJECT_ROOT / \"data/processed/rbrp_scores/merged_control.rt\"\n            merge_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/merge_rt_files.py \\\n                           -i {':'.join(control_files)} -o {merged_control}\"\"\"\n            run_command(merge_cmd, \"Merging control RTstop files\", check=False)\n    \n    # 各プローブサンプルのRBRPスコア計算\n    for sample_name in tqdm(probe_samples, desc=\"Calculating individual RBRP scores\"):\n        rt_file = find_latest_processed_files(sample_name, \"rt\")\n        \n        if not rt_file or not rt_file.exists():\n            logger.warning(f\"⚠️ {sample_name}: RTファイルが見つかりません\")\n            continue\n        \n        # RTファイル正規化\n        normalized_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_normalized.rt\"\n        norm_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/normalize_rt_file.py \\\n                      -i {rt_file} -o {normalized_file} -d 32 -l 32\"\"\"\n        run_command(norm_cmd, f\"Normalizing RT file for {sample_name}\", check=False)\n        \n        # RBRPスコア計算\n        rbrp_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_rbrp.out\"\n        background_file = merged_control if 'merged_control' in locals() else None\n        \n        if background_file and background_file.exists():\n            rbrp_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rbrp_score.py \\\n                          -f {normalized_file} -b {background_file} -o {rbrp_file} \\\n                          -e dividing -y 0.5\"\"\"\n        else:\n            rbrp_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rbrp_score.py \\\n                          -f {normalized_file} -o {rbrp_file}\"\"\"\n        \n        run_command(rbrp_cmd, f\"Calculating RBRP scores for {sample_name}\", check=False)\n        \n        # 低品質スコアフィルタリング\n        filtered_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n        filter_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/filter_rbrp_scores.py \\\n                        -i {rbrp_file} -o {filtered_file} \\\n                        -t {ANALYSIS_PARAMS['min_sequencing_depth']} -s 5 -e 30\"\"\"\n        run_command(filter_cmd, f\"Filtering RBRP scores for {sample_name}\", check=False)\n        \n        logger.info(f\"✅ {sample_name}: RBRPスコア計算完了\")\n    \n    # Replicate統合処理\n    if EXPERIMENT_DESIGN['replicate_handling']['merge_replicates']:\n        print(\"\\n🔗 Replicateマージ処理:\")\n        \n        for condition_name, condition_info in EXPERIMENT_DESIGN['conditions'].items():\n            if condition_info['type'] != 'probe':\n                continue  # プローブサンプルのみ処理\n                \n            replicate_count = len(condition_info['replicates'])\n            print(f\"\\n📊 {condition_name} (N={replicate_count}):\")\n            \n            # 外れ値検出\n            if EXPERIMENT_DESIGN['replicate_handling']['outlier_detection']:\n                outliers = detect_outliers_in_condition(condition_name, 'rbrp')\n                if outliers:\n                    print(f\"⚠️ 外れ値検出: {outliers}\")\n            \n            # RTファイルマージ\n            merged_rt_file = PROJECT_ROOT / f\"data/processed/merged_conditions/{condition_name}_merged.rt\"\n            if merge_replicate_files(condition_name, 'rt', merged_rt_file, \n                                   EXPERIMENT_DESIGN['replicate_handling']['statistical_method']):\n                print(f\"✅ RTファイルマージ完了: {condition_name}\")\n            \n            # RPKMファイルマージ\n            merged_rpkm_file = PROJECT_ROOT / f\"data/processed/merged_conditions/{condition_name}_merged.rpkm\"\n            if merge_replicate_files(condition_name, 'rpkm', merged_rpkm_file,\n                                   EXPERIMENT_DESIGN['replicate_handling']['statistical_method']):\n                print(f\"✅ RPKMファイルマージ完了: {condition_name}\")\n            \n            # RBRPスコアマージ\n            merged_rbrp_file = PROJECT_ROOT / f\"data/processed/merged_conditions/{condition_name}_merged_rbrp.out\"\n            if merge_replicate_files(condition_name, 'rbrp', merged_rbrp_file,\n                                   EXPERIMENT_DESIGN['replicate_handling']['statistical_method']):\n                print(f\"✅ RBRPスコアマージ完了: {condition_name}\")\n                \n                # マージ済みファイルの統計サマリー生成\n                stats_file = PROJECT_ROOT / f\"data/results/statistical_analysis/{condition_name}_replicate_stats.txt\"\n                stats_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/generate_replicate_stats.py \\\n                               -c {condition_name} -n {replicate_count} \\\n                               -m {EXPERIMENT_DESIGN['replicate_handling']['statistical_method']} \\\n                               -o {stats_file}\"\"\"\n                run_command(stats_cmd, f\"Generating replicate statistics for {condition_name}\", check=False)\n    \n    # 個別replicate結果の保持\n    if EXPERIMENT_DESIGN['replicate_handling']['keep_individual_results']:\n        print(\"\\n📦 個別replicate結果保持: ✅ 有効\")\n    else:\n        print(\"\\n📦 個別replicate結果保持: ❌ 無効（クリーンアップ実行）\")\n        # ここでクリーンアップロジックを追加可能\n    \n    print(\"✅ ステップ4完了: RBRPスコア計算・統計解析（Replicate対応・動的ファイル解決）\")\n\nstep4_rbrp_score_calculation()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ステップ5: 可視化ファイル生成・結果出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step5_visualization_and_output():\n    \"\"\"ステップ5: 可視化ファイル生成と結果出力\"\"\"\n    print(\"\\n📈 ステップ5: 可視化ファイル生成・結果出力開始\")\n    \n    # プローブサンプルを定義（SRA accession numbers基準）\n    probe_samples = [\n        'SRR22397001',  # HEK293_Probe2_only_rep1\n        'SRR22397002',  # HEK293_Probe2_only_rep2\n        'SRR22397003',  # HEK293_Probe2_Levofloxacin_rep1\n        'SRR22397004'   # HEK293_Probe2_Levofloxacin_rep2\n    ]\n    \n    for sample_name in tqdm(probe_samples, desc=\"Generating visualization files\"):\n        filtered_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n        \n        if not filtered_file.exists():\n            logger.warning(f\"⚠️ フィルタリング済みファイルが見つかりません: {filtered_file}\")\n            continue\n        \n        # bedgraphファイル生成\n        bedgraph_file = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}.bedgraph\"\n        bedgraph_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/generate_bedgraph.py \\\n                          -i {filtered_file} -o {bedgraph_file} \\\n                          -g {REFERENCE_FILES['genome_gtf']} \\\n                          -a {REFERENCE_FILES.get('transcriptome_fasta', '')}\"\"\"\n        run_command(bedgraph_cmd, f\"Generating bedgraph for {sample_name}\", check=False)\n        \n        # bigwigファイル生成（UCscツールが利用可能な場合）\n        bigwig_file = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}.bw\"\n        genome_size_file = PROJECT_ROOT / \"data/genome.size\"  # 事前に準備が必要\n        \n        if bedgraph_file.exists():\n            # ソートと重複除去\n            sorted_bedgraph = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}_sorted.bedgraph\"\n            sort_cmd = f\"sort -k1,1 -k2,3n {bedgraph_file} | uniq > {sorted_bedgraph}\"\n            run_command(sort_cmd, f\"Sorting bedgraph for {sample_name}\", check=False)\n            \n            # bigwig変換\n            if genome_size_file.exists():\n                bw_cmd = f\"bedGraphToBigWig {sorted_bedgraph} {genome_size_file} {bigwig_file}\"\n                run_command(bw_cmd, f\"Converting to bigwig for {sample_name}\", check=False)\n        \n        logger.info(f\"✅ {sample_name}: 可視化ファイル生成完了\")\n    \n    print(\"✅ ステップ5完了: 可視化ファイル生成・結果出力\")\n\nstep5_visualization_and_output()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 結果サマリー・統計情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 実行時間の記録\nprint(f\"\\n🎉 Replicate対応動的ファイル解決パイプライン実行完了!\")\nprint(f\"📁 結果ファイルは以下に保存されました:\")\nprint(f\"   - 個別処理済みデータ: {PROJECT_ROOT}/data/processed/\")\nprint(f\"   - 条件別マージデータ: {PROJECT_ROOT}/data/processed/merged_conditions/\")\nprint(f\"   - 統計解析結果: {PROJECT_ROOT}/data/results/statistical_analysis/\")\nprint(f\"   - 最終結果: {PROJECT_ROOT}/data/results/\")\nprint(f\"   - ログファイル: {log_file}\")\n\n# ディスク容量管理サマリー\nif PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False):\n    print(f\"\\n💾 ディスク容量管理サマリー:\")\n    print(f\"   🗑️ 中間FASTQファイル自動削除: ✅ 有効\")\n    print(f\"   🔒 元ファイル保護: ✅ 有効\")\n    print(f\"   📊 推定容量節約: 大きなFASTQファイルの場合、数GB〜数十GB節約可能\")\n    print(f\"   ⚠️ 注意: 元のダウンロード/シーケンスファイルは保護されます\")\nelse:\n    print(f\"\\n💾 ディスク容量管理:\")\n    print(f\"   🗑️ 中間FASTQファイル削除: ❌ 無効\")\n    print(f\"   ⚠️ 手動クリーンアップが必要な場合があります\")\n    \n    # 手動クリーンアップの提案\n    print(f\"\\n📋 手動クリーンアップが必要な中間ファイル:\")\n    for sample_name in INPUT_FASTQ_FILES.keys():\n        potential_cleanup_files = [\n            f\"data/processed/{sample_name}_demux_*.fastq\",\n            f\"data/processed/{sample_name}_rmdup_*.fastq\",\n            f\"data/processed/trimmed/{sample_name}_trimmed_*.fastq\"\n        ]\n        for pattern in potential_cleanup_files:\n            print(f\"   - {pattern}\")\n\nlogger.info(\"RBRP Dry Protocol Pipeline (Replicate-aware Dynamic File Resolution with Disk Management) completed successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 結果可視化（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization_plots():\n",
    "    \"\"\"結果の可視化プロット作成\"\"\"\n",
    "    print(\"\\n📊 結果可視化プロット作成\")\n",
    "    \n",
    "    # 処理サマリーの可視化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RBRP Dry Protocol - 処理結果サマリー', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. ファイルサイズ分布\n",
    "    size_columns = [col for col in summary_report.columns if 'size_mb' in col]\n",
    "    if size_columns:\n",
    "        size_data = summary_report[size_columns].fillna(0)\n",
    "        axes[0, 0].bar(range(len(size_columns)), size_data.mean(), \n",
    "                      tick_label=[col.replace('_size_mb', '') for col in size_columns])\n",
    "        axes[0, 0].set_title('平均ファイルサイズ (MB)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. 処理成功率\n",
    "    success_columns = [col for col in summary_report.columns if 'exists' in col]\n",
    "    if success_columns:\n",
    "        success_rates = summary_report[success_columns].mean() * 100\n",
    "        axes[0, 1].bar(range(len(success_rates)), success_rates.values,\n",
    "                      tick_label=[col.replace('_exists', '') for col in success_columns])\n",
    "        axes[0, 1].set_title('処理成功率 (%)')\n",
    "        axes[0, 1].set_ylim(0, 100)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. サンプル別処理状況\n",
    "    if success_columns:\n",
    "        sample_success = summary_report[success_columns].sum(axis=1)\n",
    "        axes[1, 0].bar(range(len(sample_success)), sample_success.values,\n",
    "                      tick_label=summary_report['sample_name'])\n",
    "        axes[1, 0].set_title('サンプル別完了ステップ数')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. 処理時間の目安（仮想データ）\n",
    "    processing_steps = ['デマルチプレックス', 'トリミング', 'アライメント', 'RBRP計算', '可視化']\n",
    "    estimated_times = [5, 10, 30, 20, 5]  # 分単位\n",
    "    axes[1, 1].bar(processing_steps, estimated_times)\n",
    "    axes[1, 1].set_title('ステップ別推定処理時間 (分)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 図を保存\n",
    "    plot_file = PROJECT_ROOT / \"data/results/figures/processing_summary.png\"\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📊 可視化プロット保存: {plot_file}\")\n",
    "\n",
    "# 可視化実行\n",
    "try:\n",
    "    create_visualization_plots()\n",
    "except Exception as e:\n",
    "    logger.warning(f\"可視化プロット作成エラー: {e}\")\n",
    "    print(\"⚠️ 可視化プロットの作成でエラーが発生しましたが、パイプライン自体は正常に完了しています\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}