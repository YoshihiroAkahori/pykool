{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBRP Dry Protocol - ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹è§£æè‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Eric Koolãƒ©ãƒœã®è«–æ–‡ã€ŒReactivity-based RNA profiling for analyzing transcriptome interactions of small molecules in human cellsã€ã®ãƒ‰ãƒ©ã‚¤ãƒ—ãƒ­ãƒˆã‚³ãƒ«éƒ¨åˆ†ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚\n",
    "\n",
    "**å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼**: ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹åˆå­¦è€…  \n",
    "**å…¥åŠ›**: FASTQãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒšã‚¢ã‚¨ãƒ³ãƒ‰å¯¾å¿œï¼‰  \n",
    "**å‡ºåŠ›**: RNA-è–¬ç‰©ç›¸äº’ä½œç”¨è§£æçµæœ\n",
    "\n",
    "## ğŸ”§ ãƒ¢ãƒ€ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼å¯¾å¿œ\n",
    "æœ€è¿‘ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼ï¼ˆIllumina NovaSeqã€NextSeqç­‰ï¼‰ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ãŒè‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "- **ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹**: ãƒãƒ¼ã‚³ãƒ¼ãƒ‰åˆ†é›¢æ¸ˆã¿FASTQãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›\n",
    "- **ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°**: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é…åˆ—é™¤å»æ¸ˆã¿ã®å‡ºåŠ›\n",
    "- **å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: ä½å“è³ªãƒªãƒ¼ãƒ‰é™¤å»æ¸ˆã¿ã®å‡ºåŠ›\n",
    "\n",
    "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ã¯ã€ã“ã‚Œã‚‰ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é«˜é€ŸåŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "1. è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "2. ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¿…è¦ã«å¿œã˜ã¦èª¿æ•´\n",
    "3. ã€ŒRun All Cellsã€ã§å…¨è‡ªå‹•å®Ÿè¡Œ\n",
    "4. çµæœã¯`data/results/`ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒè¨­å®šã¨åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¬ RBRP Dry Protocol Pipeline Started\n",
      "ğŸ“ Project Root: /home/akahod3f/work/kool\n",
      "ğŸ“ Log File: /home/akahod3f/work/kool/logs/rbrp_pipeline_20250917_044525.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "log_file = PROJECT_ROOT / 'logs' / f'rbrp_pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"ğŸ§¬ RBRP Dry Protocol Pipeline Started\")\n",
    "print(f\"ğŸ“ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“ Log File: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”§ å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³\nPROCESSING_OPTIONS = {\n    'skip_demultiplex': True,            # True: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—\n    'skip_adapter_trimming': True,       # True: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—\n    'skip_pcr_duplicate_removal': False, # True: PCRé‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—\n    'perform_quality_control': False,    # True: FastQCã«ã‚ˆã‚‹å“è³ªç¢ºèªã‚’å®Ÿè¡Œ\n    'adapter_sequences': {               # ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é…åˆ—ï¼ˆtrimmingå®Ÿè¡Œæ™‚ã®ã¿ä½¿ç”¨ï¼‰\n        'R1': 'AGATCGGAAGAGCGGTTCAG',\n        'R2': 'AGATCGGAAGAGCGGTTCAG'\n    },\n    # ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n    'cleanup_intermediate_fastq': True,  # True: ä¸­é–“FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‰Šé™¤ã—ã¦ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç¯€ç´„\n    'preserve_original_fastq': True,     # True: å…ƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿æŒ\n    'cleanup_confirmation': True         # True: å‰Šé™¤å‰ã«å®‰å…¨æ€§ç¢ºèª\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ãƒ»å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_external_tools():\n",
    "    \"\"\"å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã®å­˜åœ¨ç¢ºèª\"\"\"\n",
    "    required_tools = [\n",
    "        'fastqc',\n",
    "        'bowtie2',\n",
    "        'gffread',\n",
    "        'wiggletools',\n",
    "        'bedGraphToBigWig',\n",
    "        'wigToBigWig'\n",
    "    ]\n",
    "    \n",
    "    missing_tools = []\n",
    "    \n",
    "    for tool in required_tools:\n",
    "        try:\n",
    "            result = subprocess.run(['which', tool], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"âœ… {tool}: {result.stdout.strip()}\")\n",
    "            else:\n",
    "                missing_tools.append(tool)\n",
    "                print(f\"âŒ {tool}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        except Exception as e:\n",
    "            missing_tools.append(tool)\n",
    "            print(f\"âŒ {tool}: ã‚¨ãƒ©ãƒ¼ - {e}\")\n",
    "    \n",
    "    if missing_tools:\n",
    "        print(f\"\\nâš ï¸ ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_tools)}\")\n",
    "        print(\"ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯README.mdã‚’å‚ç…§ã—ã¦ãã ã•ã„\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\nğŸ‰ ã™ã¹ã¦ã®å¿…è¦ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™\")\n",
    "        return True\n",
    "\n",
    "tools_available = check_external_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ç¾¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_command(cmd, description, check=True):\n    \"\"\"ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\"\"\"\n    logger.info(f\"å®Ÿè¡Œä¸­: {description}\")\n    logger.debug(f\"ã‚³ãƒãƒ³ãƒ‰: {cmd}\")\n    \n    try:\n        result = subprocess.run(cmd, shell=True, check=check, \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            logger.info(f\"âœ… å®Œäº†: {description}\")\n            return result\n        else:\n            logger.error(f\"âŒ å¤±æ•—: {description}\")\n            logger.error(f\"ã‚¨ãƒ©ãƒ¼å‡ºåŠ›: {result.stderr}\")\n            if check:\n                raise subprocess.CalledProcessError(result.returncode, cmd)\n            return result\n    except Exception as e:\n        logger.error(f\"âŒ ä¾‹å¤–ç™ºç”Ÿ: {description} - {e}\")\n        if check:\n            raise\n        return None\n\ndef is_original_fastq_file(file_path, sample_id):\n    \"\"\"\n    å…ƒã®FASTQãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‹ã©ã†ã‹åˆ¤å®š\n    \n    Args:\n        file_path (Path): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n        sample_id (str): ã‚µãƒ³ãƒ—ãƒ«ID\n    \n    Returns:\n        bool: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆTrue\n    \"\"\"\n    if sample_id not in INPUT_FASTQ_FILES:\n        return False\n        \n    original_r1 = Path(INPUT_FASTQ_FILES[sample_id]['R1']).resolve()\n    original_r2 = Path(INPUT_FASTQ_FILES[sample_id]['R2']).resolve()\n    file_path_resolved = Path(file_path).resolve()\n    \n    return file_path_resolved == original_r1 or file_path_resolved == original_r2\n\ndef safe_cleanup_fastq_files(files_to_remove, sample_id, step_description):\n    \"\"\"\n    ä¸­é–“FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«å‰Šé™¤\n    \n    Args:\n        files_to_remove (list): å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ\n        sample_id (str): ã‚µãƒ³ãƒ—ãƒ«ID\n        step_description (str): å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜\n    \n    Returns:\n        dict: å‰Šé™¤çµæœã®çµ±è¨ˆ\n    \"\"\"\n    if not PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False):\n        return {'skipped': True, 'reason': 'cleanup disabled'}\n    \n    cleanup_stats = {\n        'files_checked': 0,\n        'files_removed': 0,\n        'space_freed_mb': 0,\n        'preserved_originals': 0,\n        'errors': []\n    }\n    \n    for file_path in files_to_remove:\n        file_path = Path(file_path)\n        cleanup_stats['files_checked'] += 1\n        \n        if not file_path.exists():\n            continue\n            \n        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¿è­·ãƒã‚§ãƒƒã‚¯\n        if PROCESSING_OPTIONS.get('preserve_original_fastq', True):\n            if is_original_fastq_file(file_path, sample_id):\n                cleanup_stats['preserved_originals'] += 1\n                logger.info(f\"ğŸ”’ å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¿è­·: {file_path.name}\")\n                continue\n        \n        # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã®å ´åˆã¯å‰Šé™¤ï¼ˆå®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã—ãªã„ï¼‰\n        if file_path.is_symlink():\n            try:\n                file_size_mb = 0  # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ãªã®ã§ã‚µã‚¤ã‚ºã¯ã‚«ã‚¦ãƒ³ãƒˆã—ãªã„\n                file_path.unlink()\n                cleanup_stats['files_removed'] += 1\n                logger.info(f\"ğŸ—‘ï¸ ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯å‰Šé™¤: {file_path.name}\")\n            except Exception as e:\n                cleanup_stats['errors'].append(f\"ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯å‰Šé™¤å¤±æ•— {file_path}: {e}\")\n            continue\n        \n        # å®‰å…¨æ€§ç¢ºèª\n        if PROCESSING_OPTIONS.get('cleanup_confirmation', True):\n            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸­é–“å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ç¢ºèª\n            is_intermediate = any(keyword in str(file_path) for keyword in \n                                ['_demux', '_rmdup', '_trimmed', 'processed/'])\n            \n            if not is_intermediate:\n                logger.warning(f\"âš ï¸ å‰Šé™¤ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã§ãªã„å¯èƒ½æ€§ï¼‰: {file_path}\")\n                continue\n        \n        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¨˜éŒ²\n        try:\n            file_size_mb = file_path.stat().st_size / (1024 * 1024)\n            \n            # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Ÿè¡Œ\n            file_path.unlink()\n            cleanup_stats['files_removed'] += 1\n            cleanup_stats['space_freed_mb'] += file_size_mb\n            \n            logger.info(f\"ğŸ—‘ï¸ å‰Šé™¤å®Œäº†: {file_path.name} ({file_size_mb:.1f} MB)\")\n            \n        except Exception as e:\n            cleanup_stats['errors'].append(f\"å‰Šé™¤å¤±æ•— {file_path}: {e}\")\n            logger.error(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {file_path} - {e}\")\n    \n    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚µãƒãƒªãƒ¼\n    if cleanup_stats['files_removed'] > 0:\n        logger.info(f\"ğŸ’¾ {step_description} - ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: \"\n                   f\"{cleanup_stats['files_removed']}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤, \"\n                   f\"{cleanup_stats['space_freed_mb']:.1f}MBç¯€ç´„\")\n    \n    return cleanup_stats\n\ndef get_condition_replicates(condition_name):\n    \"\"\"\n    å®Ÿé¨“æ¡ä»¶ã®replicateä¸€è¦§ã‚’å–å¾—\n    \n    Args:\n        condition_name (str): å®Ÿé¨“æ¡ä»¶å\n    \n    Returns:\n        list: replicate IDä¸€è¦§\n    \"\"\"\n    if condition_name in EXPERIMENT_DESIGN['conditions']:\n        return EXPERIMENT_DESIGN['conditions'][condition_name]['replicates']\n    return []\n\ndef get_samples_by_type(sample_type):\n    \"\"\"\n    ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã«replicateä¸€è¦§ã‚’å–å¾—\n    \n    Args:\n        sample_type (str): 'probe' ã¾ãŸã¯ 'control'\n    \n    Returns:\n        list: replicate IDä¸€è¦§\n    \"\"\"\n    samples = []\n    for condition_name, condition_info in EXPERIMENT_DESIGN['conditions'].items():\n        if condition_info['type'] == sample_type:\n            samples.extend(condition_info['replicates'])\n    return samples\n\ndef get_condition_from_sample(sample_id):\n    \"\"\"\n    ã‚µãƒ³ãƒ—ãƒ«IDã‹ã‚‰å®Ÿé¨“æ¡ä»¶ã‚’å–å¾—\n    \n    Args:\n        sample_id (str): ã‚µãƒ³ãƒ—ãƒ«ID\n    \n    Returns:\n        str: å®Ÿé¨“æ¡ä»¶å\n    \"\"\"\n    if sample_id in INPUT_FASTQ_FILES:\n        return INPUT_FASTQ_FILES[sample_id]['condition']\n    return None\n\ndef merge_replicate_files(condition_name, file_pattern, output_file, method='mean'):\n    \"\"\"\n    åŒä¸€æ¡ä»¶ã®replicateãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±è¨ˆçš„ã«ãƒãƒ¼ã‚¸\n    \n    Args:\n        condition_name (str): å®Ÿé¨“æ¡ä»¶å\n        file_pattern (str): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ ('rt', 'rpkm', 'rbrp' ãªã©)\n        output_file (Path): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n        method (str): çµ±è¨ˆæ‰‹æ³• ('mean', 'median', 'sum')\n    \n    Returns:\n        bool: æˆåŠŸ/å¤±æ•—\n    \"\"\"\n    replicates = get_condition_replicates(condition_name)\n    \n    if len(replicates) < EXPERIMENT_DESIGN['replicate_handling']['min_replicates']:\n        logger.warning(f\"âš ï¸ {condition_name}: replicateæ•°ä¸è¶³ ({len(replicates)} < {EXPERIMENT_DESIGN['replicate_handling']['min_replicates']})\")\n        return False\n    \n    # å„replicateã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—\n    input_files = []\n    for replicate_id in replicates:\n        if file_pattern == 'rt':\n            file_path = find_latest_processed_files(replicate_id, \"rt\")\n        elif file_pattern == 'rpkm':\n            file_path = PROJECT_ROOT / f\"data/processed/aligned/{replicate_id}.rpkm\"\n        elif file_pattern == 'rbrp':\n            file_path = PROJECT_ROOT / f\"data/processed/rbrp_scores/{replicate_id}_filtered.out\"\n        else:\n            continue\n            \n        if file_path and Path(file_path).exists():\n            input_files.append(str(file_path))\n        else:\n            logger.warning(f\"âš ï¸ {replicate_id}: {file_pattern}ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n    \n    if len(input_files) < 2:\n        logger.warning(f\"âš ï¸ {condition_name}: æœ‰åŠ¹ãª{file_pattern}ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³\")\n        return False\n    \n    # Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§replicateçµ±åˆ\n    merge_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/merge_replicates.py \\\n                   -i {':'.join(input_files)} -o {output_file} \\\n                   -m {method} -c {condition_name}\"\"\"\n    \n    result = run_command(merge_cmd, f\"Merging {file_pattern} files for {condition_name}\", check=False)\n    \n    if result and result.returncode == 0:\n        logger.info(f\"âœ… {condition_name}: {file_pattern}ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸å®Œäº† (N={len(input_files)})\")\n        return True\n    else:\n        logger.error(f\"âŒ {condition_name}: {file_pattern}ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸å¤±æ•—\")\n        return False\n\ndef detect_outliers_in_condition(condition_name, file_pattern='rpkm'):\n    \"\"\"\n    åŒä¸€æ¡ä»¶å†…ã§ã®replicateå¤–ã‚Œå€¤æ¤œå‡º\n    \n    Args:\n        condition_name (str): å®Ÿé¨“æ¡ä»¶å\n        file_pattern (str): è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³\n    \n    Returns:\n        list: å¤–ã‚Œå€¤ã¨åˆ¤å®šã•ã‚ŒãŸreplicate IDä¸€è¦§\n    \"\"\"\n    if not EXPERIMENT_DESIGN['replicate_handling']['outlier_detection']:\n        return []\n    \n    replicates = get_condition_replicates(condition_name)\n    if len(replicates) < 3:  # å¤–ã‚Œå€¤æ¤œå‡ºã«ã¯æœ€ä½3ã‚µãƒ³ãƒ—ãƒ«å¿…è¦\n        return []\n    \n    # ç°¡æ˜“çµ±è¨ˆè§£æã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡ºã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…\n    # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šè©³ç´°ãªçµ±è¨ˆè§£æãŒå¿…è¦ï¼‰\n    outliers = []\n    \n    logger.info(f\"ğŸ“Š {condition_name}: å¤–ã‚Œå€¤æ¤œå‡ºå®Ÿè¡Œä¸­ (N={len(replicates)})\")\n    # ã“ã“ã«çµ±è¨ˆè§£æãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ \n    \n    return outliers\n\ndef validate_replicate_structure():\n    \"\"\"\n    å®Ÿé¨“è¨­è¨ˆã®replicateæ§‹é€ ã‚’æ¤œè¨¼\n    \n    Returns:\n        dict: æ¤œè¨¼çµæœ\n    \"\"\"\n    validation_results = {\n        'valid': True,\n        'warnings': [],\n        'errors': [],\n        'summary': {}\n    }\n    \n    for condition_name, condition_info in EXPERIMENT_DESIGN['conditions'].items():\n        replicate_count = len(condition_info['replicates'])\n        validation_results['summary'][condition_name] = {\n            'replicate_count': replicate_count,\n            'type': condition_info['type']\n        }\n        \n        if replicate_count < EXPERIMENT_DESIGN['replicate_handling']['min_replicates']:\n            validation_results['warnings'].append(\n                f\"{condition_name}: replicateæ•°ãŒæœ€å°è¦ä»¶ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ (N={replicate_count})\"\n            )\n        \n        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n        missing_files = []\n        for replicate_id in condition_info['replicates']:\n            if replicate_id in INPUT_FASTQ_FILES:\n                r1_path = INPUT_FASTQ_FILES[replicate_id]['R1']\n                r2_path = INPUT_FASTQ_FILES[replicate_id]['R2']\n                if not Path(r1_path).exists():\n                    missing_files.append(f\"{replicate_id}_R1\")\n                if not Path(r2_path).exists():\n                    missing_files.append(f\"{replicate_id}_R2\")\n        \n        if missing_files:\n            validation_results['errors'].append(\n                f\"{condition_name}: FASTQãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {', '.join(missing_files)}\"\n            )\n            validation_results['valid'] = False\n    \n    return validation_results\n\ndef find_latest_processed_files(sample_name, file_type=\"fastq\"):\n    \"\"\"\n    ã‚µãƒ³ãƒ—ãƒ«ã®æœ€æ–°å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹•çš„ã«æ¤œç´¢\n    \n    Args:\n        sample_name (str): ã‚µãƒ³ãƒ—ãƒ«å\n        file_type (str): ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ— (\"fastq\", \"sam\", \"rt\" ãªã©)\n    \n    Returns:\n        dict: {\"R1\": path, \"R2\": path} ã¾ãŸã¯ Path ã¾ãŸã¯ None\n    \"\"\"\n    search_patterns = {\n        \"fastq\": [\n            f\"data/processed/trimmed/{sample_name}_trimmed\",  # ãƒˆãƒªãƒŸãƒ³ã‚°æ¸ˆã¿\n            f\"data/processed/{sample_name}_rmdup\",            # PCRé‡è¤‡é™¤å»æ¸ˆã¿\n            f\"data/processed/{sample_name}_demux\",            # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿\n        ],\n        \"sam\": [\n            f\"data/processed/aligned/{sample_name}.sam\"\n        ],\n        \"rt\": [\n            f\"data/processed/aligned/{sample_name}.rt\"\n        ]\n    }\n    \n    if file_type == \"fastq\":\n        # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ\n        for pattern in search_patterns[file_type]:\n            r1_file = PROJECT_ROOT / f\"{pattern}_1.fastq\"\n            r2_file = PROJECT_ROOT / f\"{pattern}_2.fastq\"\n            \n            if r1_file.exists() and r2_file.exists():\n                logger.info(f\"âœ… {sample_name}: ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ« {pattern}_*.fastq\")\n                return {\"R1\": r1_file, \"R2\": r2_file}\n        \n        # æœ€å¾Œã®æ‰‹æ®µï¼šå…ƒã®FASTQãƒ•ã‚¡ã‚¤ãƒ«\n        if sample_name in INPUT_FASTQ_FILES:\n            original_r1 = INPUT_FASTQ_FILES[sample_name]['R1']\n            original_r2 = INPUT_FASTQ_FILES[sample_name]['R2']\n            if Path(original_r1).exists() and Path(original_r2).exists():\n                logger.warning(f\"âš ï¸ {sample_name}: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ {original_r1}, {original_r2}\")\n                return {\"R1\": Path(original_r1), \"R2\": Path(original_r2)}\n            \n        return None\n    \n    else:\n        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ\n        for pattern in search_patterns.get(file_type, []):\n            file_path = PROJECT_ROOT / pattern\n            if file_path.exists():\n                logger.info(f\"âœ… {sample_name}: ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ« {pattern}\")\n                return file_path\n        return None\n\ndef create_output_dirs():\n    \"\"\"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\"\"\"\n    dirs = [\n        'logs',  # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ \n        'data/processed/fastqc',\n        'data/processed/trimmed', \n        'data/processed/aligned',\n        'data/processed/rbrp_scores',\n        'data/processed/merged_conditions',  # replicateçµ±åˆçµæœ\n        'data/results/bigwig',\n        'data/results/figures',\n        'data/results/statistical_analysis'  # çµ±è¨ˆè§£æçµæœ\n    ]\n    \n    for dir_path in dirs:\n        (PROJECT_ROOT / dir_path).mkdir(parents=True, exist_ok=True)\n    \n    logger.info(\"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ\")\n\n# å®Ÿé¨“è¨­è¨ˆã®æ¤œè¨¼å®Ÿè¡Œ\nvalidation_results = validate_replicate_structure()\ncreate_output_dirs()\n\nprint(\"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†\")\n\n# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨­å®šã®è¡¨ç¤º\nif PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False):\n    print(\"ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†: âœ… ä¸­é–“FASTQãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•å‰Šé™¤æœ‰åŠ¹\")\n    print(f\"ğŸ”’ å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¿è­·: {'âœ… æœ‰åŠ¹' if PROCESSING_OPTIONS.get('preserve_original_fastq', True) else 'âŒ ç„¡åŠ¹'}\")\nelse:\n    print(\"ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†: âŒ ä¸­é–“FASTQãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ç„¡åŠ¹ï¼ˆæ‰‹å‹•å‰Šé™¤ãŒå¿…è¦ï¼‰\")\n\nprint(\"\\nğŸ”¬ å®Ÿé¨“è¨­è¨ˆæ¤œè¨¼çµæœ:\")\nif validation_results['valid']:\n    print(\"âœ… å®Ÿé¨“è¨­è¨ˆã¯æœ‰åŠ¹ã§ã™\")\nelse:\n    print(\"âŒ å®Ÿé¨“è¨­è¨ˆã«ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Šã¾ã™\")\n\nfor warning in validation_results['warnings']:\n    print(f\"âš ï¸ è­¦å‘Š: {warning}\")\n\nfor error in validation_results['errors']:\n    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {error}\")\n\nprint(f\"\\nğŸ“Š å®Ÿé¨“è¨­è¨ˆã‚µãƒãƒªãƒ¼:\")\nfor condition, info in validation_results['summary'].items():\n    print(f\"  ğŸ§ª {condition}: N={info['replicate_count']} ({info['type']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ã‚¹ãƒ†ãƒƒãƒ—1: å“è³ªç®¡ç†ãƒ»ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_quality_control_and_demultiplex():\n",
    "    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—1: å“è³ªç®¡ç†ã¨ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"\\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—1: å“è³ªç®¡ç†ãƒ»ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹é–‹å§‹\")\n",
    "    \n",
    "    # ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª\n",
    "    skip_demux = PROCESSING_OPTIONS.get('skip_demultiplex', False)\n",
    "    perform_qc = PROCESSING_OPTIONS.get('perform_quality_control', True)\n",
    "    \n",
    "    if skip_demux:\n",
    "        print(\"ğŸš€ ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€è¿‘ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼ã§ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ï¼‰\")\n",
    "    if not perform_qc:\n",
    "        print(\"ğŸš€ å“è³ªç®¡ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "    \n",
    "    for sample_name, fastq_paths in tqdm(INPUT_FASTQ_FILES.items(), desc=\"Processing samples\"):\n",
    "        # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
    "        r1_path = fastq_paths['R1']\n",
    "        r2_path = fastq_paths['R2']\n",
    "        \n",
    "        if not Path(r1_path).exists():\n",
    "            logger.warning(f\"âš ï¸ R1ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {r1_path}\")\n",
    "            continue\n",
    "        if not Path(r2_path).exists():\n",
    "            logger.warning(f\"âš ï¸ R2ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {r2_path}\")\n",
    "            continue\n",
    "        \n",
    "        # FastQCå®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if perform_qc:\n",
    "            fastqc_cmd_r1 = f\"fastqc -o {PROJECT_ROOT}/data/processed/fastqc {r1_path}\"\n",
    "            fastqc_cmd_r2 = f\"fastqc -o {PROJECT_ROOT}/data/processed/fastqc {r2_path}\"\n",
    "            \n",
    "            run_command(fastqc_cmd_r1, f\"FastQC for {sample_name} R1\")\n",
    "            run_command(fastqc_cmd_r2, f\"FastQC for {sample_name} R2\")\n",
    "        else:\n",
    "            print(f\"â­ï¸ {sample_name}: FastQCã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ\")\n",
    "        \n",
    "        # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if not skip_demux and sample_name in BARCODES:\n",
    "            barcode = BARCODES[sample_name]\n",
    "            output_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "            output_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "            \n",
    "            # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒãƒ¼ã‚³ãƒ¼ãƒ‰æŠ½å‡º\n",
    "            demux_cmd_r1 = f\"\"\"grep -A 3 \"^@.*{barcode}\" {r1_path} | grep -v \"^--$\" > {output_r1}\"\"\"\n",
    "            demux_cmd_r2 = f\"\"\"grep -A 3 \"^@.*{barcode}\" {r2_path} | grep -v \"^--$\" > {output_r2}\"\"\"\n",
    "            \n",
    "            run_command(demux_cmd_r1, f\"Demultiplexing {sample_name} R1\", check=False)\n",
    "            run_command(demux_cmd_r2, f\"Demultiplexing {sample_name} R2\", check=False)\n",
    "            \n",
    "            logger.info(f\"âœ… {sample_name}: ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹å®Œäº†\")\n",
    "        elif skip_demux:\n",
    "            # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã€å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "            output_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n",
    "            output_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n",
    "            \n",
    "            # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯å‰Šé™¤ï¼‰\n",
    "            if output_r1.exists():\n",
    "                output_r1.unlink()\n",
    "            if output_r2.exists():\n",
    "                output_r2.unlink()\n",
    "                \n",
    "            output_r1.symlink_to(Path(r1_path).absolute())\n",
    "            output_r2.symlink_to(Path(r2_path).absolute())\n",
    "            \n",
    "            logger.info(f\"âœ… {sample_name}: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆï¼‰\")\n",
    "        else:\n",
    "            logger.warning(f\"âš ï¸ {sample_name}: ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    skip_msg = []\n",
    "    if skip_demux:\n",
    "        skip_msg.append(\"ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹\")\n",
    "    if not perform_qc:\n",
    "        skip_msg.append(\"å“è³ªç®¡ç†\")\n",
    "    \n",
    "    completion_msg = \"âœ… ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: å“è³ªç®¡ç†ãƒ»ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹\"\n",
    "    if skip_msg:\n",
    "        completion_msg += f\"ï¼ˆ{', '.join(skip_msg)}ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰\"\n",
    "    \n",
    "    print(completion_msg)\n",
    "\n",
    "step1_quality_control_and_demultiplex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ã‚¹ãƒ†ãƒƒãƒ—2: PCRé‡è¤‡é™¤å»ãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step2_remove_duplicates_and_trim():\n    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—2: PCRé‡è¤‡é™¤å»ã¨ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œãƒ»ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ï¼‰\"\"\"\n    print(\"\\nâœ‚ï¸ ã‚¹ãƒ†ãƒƒãƒ—2: PCRé‡è¤‡é™¤å»ãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°é–‹å§‹ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œãƒ»ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ï¼‰\")\n    \n    # ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª\n    skip_trimming = PROCESSING_OPTIONS.get('skip_adapter_trimming', False)\n    skip_pcr_removal = PROCESSING_OPTIONS.get('skip_pcr_duplicate_removal', False)\n    \n    if skip_trimming:\n        print(\"ğŸš€ ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœ€è¿‘ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚µãƒ¼ã§ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ï¼‰\")\n    if skip_pcr_removal:\n        print(\"ğŸš€ PCRé‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—\")\n    \n    # å…¨ä½“ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµ±è¨ˆ\n    total_cleanup_stats = {\n        'files_removed': 0,\n        'space_freed_mb': 0,\n        'preserved_originals': 0\n    }\n    \n    for sample_name in tqdm(INPUT_FASTQ_FILES.keys(), desc=\"Processing trimming\"):\n        # å‰ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹•çš„ã«æ¤œç´¢ï¼ˆãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿ï¼‰\n        input_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_1.fastq\"\n        input_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_demux_2.fastq\"\n        \n        if not input_r1.exists() or not input_r2.exists():\n            logger.warning(f\"âš ï¸ {sample_name}: ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n            original_r1 = Path(INPUT_FASTQ_FILES[sample_name]['R1'])\n            original_r2 = Path(INPUT_FASTQ_FILES[sample_name]['R2'])\n            if original_r1.exists() and original_r2.exists():\n                input_r1, input_r2 = original_r1, original_r2\n                logger.info(f\"ğŸ“„ {sample_name}: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\")\n            else:\n                logger.error(f\"âŒ {sample_name}: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n                continue\n        \n        # PCRé‡è¤‡é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n        files_to_cleanup_after_pcr = []\n        if not skip_pcr_removal:\n            rmdup_r1 = PROJECT_ROOT / f\"data/processed/{sample_name}_rmdup_1.fastq\"\n            rmdup_r2 = PROJECT_ROOT / f\"data/processed/{sample_name}_rmdup_2.fastq\"\n            \n            # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰PCRé‡è¤‡é™¤å»ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n            rmdup_cmd_r1 = f\"\"\"awk '/^@/ {{if (seen[$0]++) next}} 1' {input_r1} > {rmdup_r1}\"\"\"\n            rmdup_cmd_r2 = f\"\"\"awk '/^@/ {{if (seen[$0]++) next}} 1' {input_r2} > {rmdup_r2}\"\"\"\n            \n            run_command(rmdup_cmd_r1, f\"Remove duplicates for {sample_name} R1\")\n            run_command(rmdup_cmd_r2, f\"Remove duplicates for {sample_name} R2\")\n            \n            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°\n            trim_input_r1 = rmdup_r1\n            trim_input_r2 = rmdup_r2\n            \n            # ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã«è¿½åŠ ï¼ˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã§ãªã„å ´åˆï¼‰\n            if not is_original_fastq_file(input_r1, sample_name):\n                files_to_cleanup_after_pcr.append(input_r1)\n            if not is_original_fastq_file(input_r2, sample_name):\n                files_to_cleanup_after_pcr.append(input_r2)\n        else:\n            print(f\"â­ï¸ {sample_name}: PCRé‡è¤‡é™¤å»ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ\")\n            trim_input_r1 = input_r1\n            trim_input_r2 = input_r2\n        \n        # ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n        trimmed_r1 = PROJECT_ROOT / f\"data/processed/trimmed/{sample_name}_trimmed_1.fastq\"\n        trimmed_r2 = PROJECT_ROOT / f\"data/processed/trimmed/{sample_name}_trimmed_2.fastq\"\n        \n        files_to_cleanup_after_trim = []\n        if not skip_trimming:\n            # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é™¤å»ã¨ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒˆãƒªãƒŸãƒ³ã‚°\n            adapter_r1 = PROCESSING_OPTIONS['adapter_sequences']['R1']\n            adapter_r2 = PROCESSING_OPTIONS['adapter_sequences']['R2']\n            \n            trim_cmd = f\"\"\"cutadapt -a {adapter_r1} -A {adapter_r2} \\\n                          -q 30 -m {ANALYSIS_PARAMS['min_read_length']} \\\n                          -o {trimmed_r1} -p {trimmed_r2} \\\n                          {trim_input_r1} {trim_input_r2}\"\"\"\n            \n            run_command(trim_cmd, f\"Paired-end adapter trimming for {sample_name}\", check=False)\n            \n            # PCRé‡è¤‡é™¤å»æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã«è¿½åŠ \n            if not skip_pcr_removal:\n                files_to_cleanup_after_trim.extend([trim_input_r1, trim_input_r2])\n        else:\n            # ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å ´åˆã€ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n            if trimmed_r1.exists():\n                trimmed_r1.unlink()\n            if trimmed_r2.exists():\n                trimmed_r2.unlink()\n                \n            trimmed_r1.symlink_to(trim_input_r1.absolute())\n            trimmed_r2.symlink_to(trim_input_r2.absolute())\n            \n            print(f\"â­ï¸ {sample_name}: ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆï¼‰\")\n        \n        # PCRé‡è¤‡é™¤å»å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n        if files_to_cleanup_after_pcr:\n            cleanup_stats = safe_cleanup_fastq_files(\n                files_to_cleanup_after_pcr, \n                sample_name, \n                f\"PCRé‡è¤‡é™¤å»å®Œäº†å¾Œ ({sample_name})\"\n            )\n            if not cleanup_stats.get('skipped', False):\n                total_cleanup_stats['files_removed'] += cleanup_stats['files_removed']\n                total_cleanup_stats['space_freed_mb'] += cleanup_stats['space_freed_mb']\n                total_cleanup_stats['preserved_originals'] += cleanup_stats['preserved_originals']\n        \n        # ãƒˆãƒªãƒŸãƒ³ã‚°å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n        if files_to_cleanup_after_trim:\n            cleanup_stats = safe_cleanup_fastq_files(\n                files_to_cleanup_after_trim, \n                sample_name, \n                f\"ãƒˆãƒªãƒŸãƒ³ã‚°å®Œäº†å¾Œ ({sample_name})\"\n            )\n            if not cleanup_stats.get('skipped', False):\n                total_cleanup_stats['files_removed'] += cleanup_stats['files_removed']\n                total_cleanup_stats['space_freed_mb'] += cleanup_stats['space_freed_mb']\n                total_cleanup_stats['preserved_originals'] += cleanup_stats['preserved_originals']\n        \n        logger.info(f\"âœ… {sample_name}: å‡¦ç†å®Œäº†\")\n    \n    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ\n    skip_msg = []\n    if skip_trimming:\n        skip_msg.append(\"ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒˆãƒªãƒŸãƒ³ã‚°\")\n    if skip_pcr_removal:\n        skip_msg.append(\"PCRé‡è¤‡é™¤å»\")\n    \n    completion_msg = \"âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: PCRé‡è¤‡é™¤å»ãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œãƒ»ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ï¼‰\"\n    if skip_msg:\n        completion_msg += f\"ï¼ˆ{', '.join(skip_msg)}ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰\"\n    \n    print(completion_msg)\n    \n    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚µãƒãƒªãƒ¼è¡¨ç¤º\n    if PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False) and total_cleanup_stats['files_removed'] > 0:\n        print(f\"ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—2ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚µãƒãƒªãƒ¼:\")\n        print(f\"   ğŸ—‘ï¸ å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_cleanup_stats['files_removed']}\")\n        print(f\"   ğŸ’¾ ç¯€ç´„å®¹é‡: {total_cleanup_stats['space_freed_mb']:.1f} MB\")\n        print(f\"   ğŸ”’ ä¿è­·ã—ãŸå…ƒãƒ•ã‚¡ã‚¤ãƒ«: {total_cleanup_stats['preserved_originals']}\")\n\nstep2_remove_duplicates_and_trim()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ã‚¹ãƒ†ãƒƒãƒ—3: é…åˆ—ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»è»¢å†™ç”£ç‰©ç™ºç¾é‡è¨ˆç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3_alignment_and_expression():\n",
    "    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—3: é…åˆ—ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨è»¢å†™ç”£ç‰©ç™ºç¾é‡è¨ˆç®—ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\"\"\"\n",
    "    print(\"\\nğŸ§¬ ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»ç™ºç¾é‡è¨ˆç®—é–‹å§‹ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n",
    "    \n",
    "    for sample_name in tqdm(INPUT_FASTQ_FILES.keys(), desc=\"Processing alignment\"):\n",
    "        # å‹•çš„ã«æœ€æ–°ã®å‡¦ç†æ¸ˆã¿FASTQãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢\n",
    "        fastq_files = find_latest_processed_files(sample_name, \"fastq\")\n",
    "        \n",
    "        if not fastq_files:\n",
    "            logger.error(f\"âŒ {sample_name}: å‡¦ç†æ¸ˆã¿FASTQãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            continue\n",
    "            \n",
    "        trimmed_r1 = fastq_files[\"R1\"]\n",
    "        trimmed_r2 = fastq_files[\"R2\"]\n",
    "        \n",
    "        if not trimmed_r1.exists():\n",
    "            logger.warning(f\"âš ï¸ R1ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {trimmed_r1}\")\n",
    "            continue\n",
    "        if not trimmed_r2.exists():\n",
    "            logger.warning(f\"âš ï¸ R2ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {trimmed_r2}\")\n",
    "            continue\n",
    "        \n",
    "        # Bowtie2ã§ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ\n",
    "        sam_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.sam\"\n",
    "        \n",
    "        # ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ-1, -2ã§ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šï¼‰\n",
    "        align_cmd = f\"\"\"bowtie2 -1 {trimmed_r1} -2 {trimmed_r2} -S {sam_file} \\\n",
    "                       -x {REFERENCE_FILES['transcriptome_index']} \\\n",
    "                       --non-deterministic --time \\\n",
    "                       --minins 50 --maxins 500 \\\n",
    "                       --no-mixed --no-discordant\"\"\"\n",
    "        \n",
    "        run_command(align_cmd, f\"Paired-end alignment for {sample_name}\")\n",
    "        \n",
    "        # RPKMè¨ˆç®—\n",
    "        rpkm_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.rpkm\"\n",
    "        rpkm_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rpkm.py -i {sam_file} -o {rpkm_file}\"\"\"\n",
    "        \n",
    "        run_command(rpkm_cmd, f\"RPKM calculation for {sample_name}\", check=False)\n",
    "        \n",
    "        # RTstopè¨ˆç®—\n",
    "        rt_file = PROJECT_ROOT / f\"data/processed/aligned/{sample_name}.rt\"\n",
    "        rt_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rtstops.py \\\n",
    "                    -i {sam_file} -o {rt_file} -r {rpkm_file} -c {ANALYSIS_PARAMS['min_rpkm']}\"\"\"\n",
    "        \n",
    "        run_command(rt_cmd, f\"RTstop calculation for {sample_name}\", check=False)\n",
    "        logger.info(f\"âœ… {sample_name}: ãƒšã‚¢ã‚¨ãƒ³ãƒ‰ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»ç™ºç¾é‡è¨ˆç®—å®Œäº†\")\n",
    "    \n",
    "    print(\"âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ»ç™ºç¾é‡è¨ˆç®—ï¼ˆå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¯¾å¿œï¼‰\")\n",
    "\n",
    "step3_alignment_and_expression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ã‚¹ãƒ†ãƒƒãƒ—4: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ»çµ±è¨ˆè§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step4_rbrp_score_calculation():\n    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—4: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ã¨çµ±è¨ˆè§£æï¼ˆReplicateå¯¾å¿œãƒ»å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºï¼‰\"\"\"\n    print(\"\\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ»çµ±è¨ˆè§£æé–‹å§‹ï¼ˆReplicateå¯¾å¿œãƒ»å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºï¼‰\")\n    \n    # å‹•çš„ã«ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—\n    probe_samples = get_samples_by_type('probe')\n    control_samples = get_samples_by_type('control')\n    \n    print(f\"ğŸ“‹ ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«: {probe_samples}\")\n    print(f\"ğŸ“‹ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚µãƒ³ãƒ—ãƒ«: {control_samples}\")\n    \n    # ã¾ãšå€‹åˆ¥replicateã®RBRPã‚¹ã‚³ã‚¢è¨ˆç®—\n    print(\"\\nğŸ”¬ å€‹åˆ¥replicateã®RBRPã‚¹ã‚³ã‚¢è¨ˆç®—:\")\n    \n    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰RTstopãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸ï¼ˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚µãƒ³ãƒ—ãƒ«ï¼‰\n    if len(control_samples) >= 2:\n        control_files = []\n        for sample in control_samples:\n            rt_file = find_latest_processed_files(sample, \"rt\")\n            if rt_file and rt_file.exists():\n                control_files.append(str(rt_file))\n            else:\n                logger.warning(f\"âš ï¸ {sample}: RTãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n        \n        if control_files:\n            merged_control = PROJECT_ROOT / \"data/processed/rbrp_scores/merged_control.rt\"\n            merge_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/merge_rt_files.py \\\n                           -i {':'.join(control_files)} -o {merged_control}\"\"\"\n            run_command(merge_cmd, \"Merging control RTstop files\", check=False)\n    \n    # å„ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®RBRPã‚¹ã‚³ã‚¢è¨ˆç®—\n    for sample_name in tqdm(probe_samples, desc=\"Calculating individual RBRP scores\"):\n        rt_file = find_latest_processed_files(sample_name, \"rt\")\n        \n        if not rt_file or not rt_file.exists():\n            logger.warning(f\"âš ï¸ {sample_name}: RTãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n            continue\n        \n        # RTãƒ•ã‚¡ã‚¤ãƒ«æ­£è¦åŒ–\n        normalized_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_normalized.rt\"\n        norm_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/normalize_rt_file.py \\\n                      -i {rt_file} -o {normalized_file} -d 32 -l 32\"\"\"\n        run_command(norm_cmd, f\"Normalizing RT file for {sample_name}\", check=False)\n        \n        # RBRPã‚¹ã‚³ã‚¢è¨ˆç®—\n        rbrp_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_rbrp.out\"\n        background_file = merged_control if 'merged_control' in locals() else None\n        \n        if background_file and background_file.exists():\n            rbrp_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rbrp_score.py \\\n                          -f {normalized_file} -b {background_file} -o {rbrp_file} \\\n                          -e dividing -y 0.5\"\"\"\n        else:\n            rbrp_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/calculate_rbrp_score.py \\\n                          -f {normalized_file} -o {rbrp_file}\"\"\"\n        \n        run_command(rbrp_cmd, f\"Calculating RBRP scores for {sample_name}\", check=False)\n        \n        # ä½å“è³ªã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n        filtered_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n        filter_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/filter_rbrp_scores.py \\\n                        -i {rbrp_file} -o {filtered_file} \\\n                        -t {ANALYSIS_PARAMS['min_sequencing_depth']} -s 5 -e 30\"\"\"\n        run_command(filter_cmd, f\"Filtering RBRP scores for {sample_name}\", check=False)\n        \n        logger.info(f\"âœ… {sample_name}: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—å®Œäº†\")\n    \n    # Replicateçµ±åˆå‡¦ç†\n    if EXPERIMENT_DESIGN['replicate_handling']['merge_replicates']:\n        print(\"\\nğŸ”— Replicateãƒãƒ¼ã‚¸å‡¦ç†:\")\n        \n        for condition_name, condition_info in EXPERIMENT_DESIGN['conditions'].items():\n            if condition_info['type'] != 'probe':\n                continue  # ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã®ã¿å‡¦ç†\n                \n            replicate_count = len(condition_info['replicates'])\n            print(f\"\\nğŸ“Š {condition_name} (N={replicate_count}):\")\n            \n            # å¤–ã‚Œå€¤æ¤œå‡º\n            if EXPERIMENT_DESIGN['replicate_handling']['outlier_detection']:\n                outliers = detect_outliers_in_condition(condition_name, 'rbrp')\n                if outliers:\n                    print(f\"âš ï¸ å¤–ã‚Œå€¤æ¤œå‡º: {outliers}\")\n            \n            # RTãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸\n            merged_rt_file = PROJECT_ROOT / f\"data/processed/merged_conditions/{condition_name}_merged.rt\"\n            if merge_replicate_files(condition_name, 'rt', merged_rt_file, \n                                   EXPERIMENT_DESIGN['replicate_handling']['statistical_method']):\n                print(f\"âœ… RTãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸å®Œäº†: {condition_name}\")\n            \n            # RPKMãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸\n            merged_rpkm_file = PROJECT_ROOT / f\"data/processed/merged_conditions/{condition_name}_merged.rpkm\"\n            if merge_replicate_files(condition_name, 'rpkm', merged_rpkm_file,\n                                   EXPERIMENT_DESIGN['replicate_handling']['statistical_method']):\n                print(f\"âœ… RPKMãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¸å®Œäº†: {condition_name}\")\n            \n            # RBRPã‚¹ã‚³ã‚¢ãƒãƒ¼ã‚¸\n            merged_rbrp_file = PROJECT_ROOT / f\"data/processed/merged_conditions/{condition_name}_merged_rbrp.out\"\n            if merge_replicate_files(condition_name, 'rbrp', merged_rbrp_file,\n                                   EXPERIMENT_DESIGN['replicate_handling']['statistical_method']):\n                print(f\"âœ… RBRPã‚¹ã‚³ã‚¢ãƒãƒ¼ã‚¸å®Œäº†: {condition_name}\")\n                \n                # ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ\n                stats_file = PROJECT_ROOT / f\"data/results/statistical_analysis/{condition_name}_replicate_stats.txt\"\n                stats_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/generate_replicate_stats.py \\\n                               -c {condition_name} -n {replicate_count} \\\n                               -m {EXPERIMENT_DESIGN['replicate_handling']['statistical_method']} \\\n                               -o {stats_file}\"\"\"\n                run_command(stats_cmd, f\"Generating replicate statistics for {condition_name}\", check=False)\n    \n    # å€‹åˆ¥replicateçµæœã®ä¿æŒ\n    if EXPERIMENT_DESIGN['replicate_handling']['keep_individual_results']:\n        print(\"\\nğŸ“¦ å€‹åˆ¥replicateçµæœä¿æŒ: âœ… æœ‰åŠ¹\")\n    else:\n        print(\"\\nğŸ“¦ å€‹åˆ¥replicateçµæœä¿æŒ: âŒ ç„¡åŠ¹ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œï¼‰\")\n        # ã“ã“ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ å¯èƒ½\n    \n    print(\"âœ… ã‚¹ãƒ†ãƒƒãƒ—4å®Œäº†: RBRPã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ»çµ±è¨ˆè§£æï¼ˆReplicateå¯¾å¿œãƒ»å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºï¼‰\")\n\nstep4_rbrp_score_calculation()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ã‚¹ãƒ†ãƒƒãƒ—5: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµæœå‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def step5_visualization_and_output():\n    \"\"\"ã‚¹ãƒ†ãƒƒãƒ—5: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã¨çµæœå‡ºåŠ›\"\"\"\n    print(\"\\nğŸ“ˆ ã‚¹ãƒ†ãƒƒãƒ—5: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµæœå‡ºåŠ›é–‹å§‹\")\n    \n    # ãƒ—ãƒ­ãƒ¼ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’å®šç¾©ï¼ˆSRA accession numbersåŸºæº–ï¼‰\n    probe_samples = [\n        'SRR22397001',  # HEK293_Probe2_only_rep1\n        'SRR22397002',  # HEK293_Probe2_only_rep2\n        'SRR22397003',  # HEK293_Probe2_Levofloxacin_rep1\n        'SRR22397004'   # HEK293_Probe2_Levofloxacin_rep2\n    ]\n    \n    for sample_name in tqdm(probe_samples, desc=\"Generating visualization files\"):\n        filtered_file = PROJECT_ROOT / f\"data/processed/rbrp_scores/{sample_name}_filtered.out\"\n        \n        if not filtered_file.exists():\n            logger.warning(f\"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filtered_file}\")\n            continue\n        \n        # bedgraphãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ\n        bedgraph_file = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}.bedgraph\"\n        bedgraph_cmd = f\"\"\"python {PROJECT_ROOT}/scripts/generate_bedgraph.py \\\n                          -i {filtered_file} -o {bedgraph_file} \\\n                          -g {REFERENCE_FILES['genome_gtf']} \\\n                          -a {REFERENCE_FILES.get('transcriptome_fasta', '')}\"\"\"\n        run_command(bedgraph_cmd, f\"Generating bedgraph for {sample_name}\", check=False)\n        \n        # bigwigãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆUCscãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n        bigwig_file = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}.bw\"\n        genome_size_file = PROJECT_ROOT / \"data/genome.size\"  # äº‹å‰ã«æº–å‚™ãŒå¿…è¦\n        \n        if bedgraph_file.exists():\n            # ã‚½ãƒ¼ãƒˆã¨é‡è¤‡é™¤å»\n            sorted_bedgraph = PROJECT_ROOT / f\"data/results/bigwig/{sample_name}_sorted.bedgraph\"\n            sort_cmd = f\"sort -k1,1 -k2,3n {bedgraph_file} | uniq > {sorted_bedgraph}\"\n            run_command(sort_cmd, f\"Sorting bedgraph for {sample_name}\", check=False)\n            \n            # bigwigå¤‰æ›\n            if genome_size_file.exists():\n                bw_cmd = f\"bedGraphToBigWig {sorted_bedgraph} {genome_size_file} {bigwig_file}\"\n                run_command(bw_cmd, f\"Converting to bigwig for {sample_name}\", check=False)\n        \n        logger.info(f\"âœ… {sample_name}: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†\")\n    \n    print(\"âœ… ã‚¹ãƒ†ãƒƒãƒ—5å®Œäº†: å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãƒ»çµæœå‡ºåŠ›\")\n\nstep5_visualization_and_output()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. çµæœã‚µãƒãƒªãƒ¼ãƒ»çµ±è¨ˆæƒ…å ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å®Ÿè¡Œæ™‚é–“ã®è¨˜éŒ²\nprint(f\"\\nğŸ‰ Replicateå¯¾å¿œå‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå®Œäº†!\")\nprint(f\"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:\")\nprint(f\"   - å€‹åˆ¥å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿: {PROJECT_ROOT}/data/processed/\")\nprint(f\"   - æ¡ä»¶åˆ¥ãƒãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿: {PROJECT_ROOT}/data/processed/merged_conditions/\")\nprint(f\"   - çµ±è¨ˆè§£æçµæœ: {PROJECT_ROOT}/data/results/statistical_analysis/\")\nprint(f\"   - æœ€çµ‚çµæœ: {PROJECT_ROOT}/data/results/\")\nprint(f\"   - ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}\")\n\n# ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ã‚µãƒãƒªãƒ¼\nif PROCESSING_OPTIONS.get('cleanup_intermediate_fastq', False):\n    print(f\"\\nğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†ã‚µãƒãƒªãƒ¼:\")\n    print(f\"   ğŸ—‘ï¸ ä¸­é–“FASTQãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•å‰Šé™¤: âœ… æœ‰åŠ¹\")\n    print(f\"   ğŸ”’ å…ƒãƒ•ã‚¡ã‚¤ãƒ«ä¿è­·: âœ… æœ‰åŠ¹\")\n    print(f\"   ğŸ“Š æ¨å®šå®¹é‡ç¯€ç´„: å¤§ããªFASTQãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€æ•°GBã€œæ•°åGBç¯€ç´„å¯èƒ½\")\n    print(f\"   âš ï¸ æ³¨æ„: å…ƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿è­·ã•ã‚Œã¾ã™\")\nelse:\n    print(f\"\\nğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ç®¡ç†:\")\n    print(f\"   ğŸ—‘ï¸ ä¸­é–“FASTQãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: âŒ ç„¡åŠ¹\")\n    print(f\"   âš ï¸ æ‰‹å‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™\")\n    \n    # æ‰‹å‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ææ¡ˆ\n    print(f\"\\nğŸ“‹ æ‰‹å‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå¿…è¦ãªä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«:\")\n    for sample_name in INPUT_FASTQ_FILES.keys():\n        potential_cleanup_files = [\n            f\"data/processed/{sample_name}_demux_*.fastq\",\n            f\"data/processed/{sample_name}_rmdup_*.fastq\",\n            f\"data/processed/trimmed/{sample_name}_trimmed_*.fastq\"\n        ]\n        for pattern in potential_cleanup_files:\n            print(f\"   - {pattern}\")\n\nlogger.info(\"RBRP Dry Protocol Pipeline (Replicate-aware Dynamic File Resolution with Disk Management) completed successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. çµæœå¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization_plots():\n",
    "    \"\"\"çµæœã®å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ\"\"\"\n",
    "    print(\"\\nğŸ“Š çµæœå¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ\")\n",
    "    \n",
    "    # å‡¦ç†ã‚µãƒãƒªãƒ¼ã®å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RBRP Dry Protocol - å‡¦ç†çµæœã‚µãƒãƒªãƒ¼', fontsize=16, y=0.98)\n",
    "    \n",
    "    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†å¸ƒ\n",
    "    size_columns = [col for col in summary_report.columns if 'size_mb' in col]\n",
    "    if size_columns:\n",
    "        size_data = summary_report[size_columns].fillna(0)\n",
    "        axes[0, 0].bar(range(len(size_columns)), size_data.mean(), \n",
    "                      tick_label=[col.replace('_size_mb', '') for col in size_columns])\n",
    "        axes[0, 0].set_title('å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º (MB)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. å‡¦ç†æˆåŠŸç‡\n",
    "    success_columns = [col for col in summary_report.columns if 'exists' in col]\n",
    "    if success_columns:\n",
    "        success_rates = summary_report[success_columns].mean() * 100\n",
    "        axes[0, 1].bar(range(len(success_rates)), success_rates.values,\n",
    "                      tick_label=[col.replace('_exists', '') for col in success_columns])\n",
    "        axes[0, 1].set_title('å‡¦ç†æˆåŠŸç‡ (%)')\n",
    "        axes[0, 1].set_ylim(0, 100)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. ã‚µãƒ³ãƒ—ãƒ«åˆ¥å‡¦ç†çŠ¶æ³\n",
    "    if success_columns:\n",
    "        sample_success = summary_report[success_columns].sum(axis=1)\n",
    "        axes[1, 0].bar(range(len(sample_success)), sample_success.values,\n",
    "                      tick_label=summary_report['sample_name'])\n",
    "        axes[1, 0].set_title('ã‚µãƒ³ãƒ—ãƒ«åˆ¥å®Œäº†ã‚¹ãƒ†ãƒƒãƒ—æ•°')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. å‡¦ç†æ™‚é–“ã®ç›®å®‰ï¼ˆä»®æƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "    processing_steps = ['ãƒ‡ãƒãƒ«ãƒãƒ—ãƒ¬ãƒƒã‚¯ã‚¹', 'ãƒˆãƒªãƒŸãƒ³ã‚°', 'ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ', 'RBRPè¨ˆç®—', 'å¯è¦–åŒ–']\n",
    "    estimated_times = [5, 10, 30, 20, 5]  # åˆ†å˜ä½\n",
    "    axes[1, 1].bar(processing_steps, estimated_times)\n",
    "    axes[1, 1].set_title('ã‚¹ãƒ†ãƒƒãƒ—åˆ¥æ¨å®šå‡¦ç†æ™‚é–“ (åˆ†)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # å›³ã‚’ä¿å­˜\n",
    "    plot_file = PROJECT_ROOT / \"data/results/figures/processing_summary.png\"\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ“Š å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {plot_file}\")\n",
    "\n",
    "# å¯è¦–åŒ–å®Ÿè¡Œ\n",
    "try:\n",
    "    create_visualization_plots()\n",
    "except Exception as e:\n",
    "    logger.warning(f\"å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"âš ï¸ å¯è¦–åŒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªä½“ã¯æ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã¾ã™\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}